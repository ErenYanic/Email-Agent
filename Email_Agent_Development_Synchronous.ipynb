{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEUU9zFSAfwb"
      },
      "outputs": [],
      "source": [
        "%pip install python-dotenv langchain langchain_community langchain-openai langsmith langgraph faiss-cpu beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Core imports and configuration**"
      ],
      "metadata": {
        "id": "RBwm4_L0aDgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Core imports and configuration\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import operator\n",
        "from typing import List, Dict, Any, Optional, Tuple, Annotated, TypedDict, Sequence\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.tools import tool, BaseTool\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage\n",
        "from langchain.schema import Document\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Email processing imports\n",
        "import base64\n",
        "from dateutil.parser import parse\n",
        "from langchain_community.agent_toolkits import GmailToolkit\n",
        "from langchain_community.tools.gmail.utils import build_resource_service\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure environment variables\n",
        "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
        "\n",
        "# Get the OpenAI API Key from the .env file\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "else:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in .env file!\")\n",
        "\n",
        "# To test the LangSmith connection\n",
        "try:\n",
        "    from langsmith import Client\n",
        "    client = Client()\n",
        "    print(\"LangSmith connection successful!\")\n",
        "    print(f\"Proje: {os.environ.get('LANGSMITH_PROJECT')}\")\n",
        "except Exception as e:\n",
        "    print(f\"LangSmith connection error: {e}\")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "test_llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Send test message (to check tracing)\n",
        "try:\n",
        "    response = test_llm.invoke(\"Hi, this is a test message!\")\n",
        "    print(\"LLM test successful!\")\n",
        "    print(\"You can check the traces in LangSmith.\")\n",
        "    print(response)\n",
        "    del test_llm  # We are deleting the test_llm to clear the memory\n",
        "except Exception as e:\n",
        "    print(f\"LLM test error: {e}\")\n",
        "\n",
        "print(\"‚úÖ Core configuration completed\")"
      ],
      "metadata": {
        "id": "cF0ESe5Atzj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Email Fetcher Class**"
      ],
      "metadata": {
        "id": "sTDzUW86aqyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Email Fetcher class - structured approach for email retrieval\n",
        "class EmailFetcher:\n",
        "    \"\"\"Gmail API email fetcher and processor class\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialise Gmail API service\"\"\"\n",
        "        self.api_resource = build_resource_service()\n",
        "        self.toolkit = GmailToolkit(api_resource=self.api_resource)\n",
        "        self.search_tool = next(\n",
        "            (tool for tool in self.toolkit.get_tools() if tool.name == 'search_gmail'),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if not self.search_tool:\n",
        "            raise ValueError(\"Gmail search tool could not be initialised!\")\n",
        "\n",
        "    def get_email_contents(self, payload: Dict) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Recursively extract content from email payload\n",
        "\n",
        "        Args:\n",
        "            payload: Email payload from Gmail API\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing 'text' and 'html' content\n",
        "        \"\"\"\n",
        "        plain_text = \"\"\n",
        "        html_text = \"\"\n",
        "\n",
        "        # Recursively process parts if they exist\n",
        "        if 'parts' in payload:\n",
        "            for part in payload['parts']:\n",
        "                nested_content = self.get_email_contents(part)\n",
        "                plain_text += nested_content['text']\n",
        "                html_text += nested_content['html']\n",
        "\n",
        "        # Decode body if present\n",
        "        elif 'body' in payload and 'data' in payload['body']:\n",
        "            mime_type = payload.get('mimeType', '')\n",
        "            body_data = payload['body'].get('data', '')\n",
        "\n",
        "            if body_data:\n",
        "                try:\n",
        "                    decoded_body = base64.urlsafe_b64decode(body_data).decode('utf-8', errors='ignore')\n",
        "                    if 'text/plain' in mime_type:\n",
        "                        plain_text += decoded_body\n",
        "                    elif 'text/html' in mime_type:\n",
        "                        html_text += decoded_body\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Decode error: {e}\")\n",
        "                    pass\n",
        "\n",
        "        return {'text': plain_text.strip(), 'html': html_text.strip()}\n",
        "\n",
        "    def fetch_emails(self, query: str = \"in:inbox\", max_results: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch emails from Gmail and return as DataFrame\n",
        "\n",
        "        Args:\n",
        "            query: Gmail search query\n",
        "            max_results: Maximum number of emails to fetch\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Email data\n",
        "        \"\"\"\n",
        "        processed_emails = []\n",
        "\n",
        "        try:\n",
        "            # Search for emails\n",
        "            search_params = {\"query\": query, \"max_results\": max_results}\n",
        "            search_results = self.search_tool.run(search_params)\n",
        "\n",
        "            print(f\"üìß Found {len(search_results)} emails, fetching details...\")\n",
        "\n",
        "            # Process each email\n",
        "            for summary in search_results:\n",
        "                message_id = summary.get('id')\n",
        "                message_detail = self.api_resource.users().messages().get(\n",
        "                    userId='me',\n",
        "                    id=message_id,\n",
        "                    format='full'\n",
        "                ).execute()\n",
        "\n",
        "                payload = message_detail.get('payload', {})\n",
        "                headers = payload.get('headers', [])\n",
        "\n",
        "                # Extract header information\n",
        "                subject = next((h['value'] for h in headers if h['name'].lower() == 'subject'), 'N/A')\n",
        "                sender = next((h['value'] for h in headers if h['name'].lower() == 'from'), 'N/A')\n",
        "                to = next((h['value'] for h in headers if h['name'].lower() == 'to'), 'N/A')\n",
        "                cc = next((h['value'] for h in headers if h['name'].lower() == 'cc'), 'N/A')\n",
        "                date_str = next((h['value'] for h in headers if h['name'].lower() == 'date'), None)\n",
        "\n",
        "                # Parse date\n",
        "                try:\n",
        "                    email_date = parse(date_str) if date_str else None\n",
        "                except:\n",
        "                    email_date = None\n",
        "\n",
        "                # Check labels and read status\n",
        "                labels = message_detail.get('labelIds', [])\n",
        "                is_unread = 'UNREAD' in labels\n",
        "\n",
        "                # Extract content - FULL TEXT, no truncation for security\n",
        "                contents = self.get_email_contents(payload)\n",
        "\n",
        "                # Recursive attachment check\n",
        "                has_attachment = False\n",
        "                attachment_names = []\n",
        "\n",
        "                def check_attachments(part):\n",
        "                    \"\"\"Recursive attachment check\"\"\"\n",
        "                    if part.get('filename'):\n",
        "                        return True, part.get('filename')\n",
        "                    if 'parts' in part:\n",
        "                        for subpart in part['parts']:\n",
        "                            has_att, filename = check_attachments(subpart)\n",
        "                            if has_att:\n",
        "                                return True, filename\n",
        "                    return False, None\n",
        "\n",
        "                if 'parts' in payload:\n",
        "                    for part in payload['parts']:\n",
        "                        has_att, filename = check_attachments(part)\n",
        "                        if has_att:\n",
        "                            has_attachment = True\n",
        "                            if filename:\n",
        "                                attachment_names.append(filename)\n",
        "\n",
        "                # Add email data - storing FULL content\n",
        "                processed_emails.append({\n",
        "                    'id': message_id,\n",
        "                    'is_unread': is_unread,\n",
        "                    'date': email_date,\n",
        "                    'from': sender,\n",
        "                    'to': to,\n",
        "                    'cc': cc,\n",
        "                    'labels': labels,\n",
        "                    'subject': subject,\n",
        "                    'body_text': contents['text'],  # Full text for analysis\n",
        "                    'body_html': contents['html'],  # Full HTML for security checks\n",
        "                    'has_attachment': has_attachment,\n",
        "                    'attachment_names': attachment_names\n",
        "                })\n",
        "\n",
        "            # Create DataFrame\n",
        "            if processed_emails:\n",
        "                df = pd.DataFrame(processed_emails)\n",
        "                df = df.sort_values(by='date', ascending=False, na_position='last').reset_index(drop=True)\n",
        "                print(f\"‚úÖ Successfully processed {len(df)} emails\")\n",
        "                return df\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No emails found\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Email fetch error: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    # Added to the code for testing purposes, to better examine the emails received and their content.\n",
        "    def save_to_csv(self, df: pd.DataFrame, filename: str = 'fetched_emails.csv'):\n",
        "        \"\"\"\n",
        "        Save DataFrame to CSV file\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to save\n",
        "            filename: Output filename\n",
        "        \"\"\"\n",
        "        if not df.empty:\n",
        "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "            print(f\"‚úÖ DataFrame successfully saved to '{filename}'\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No data to save\")\n",
        "\n",
        "# Test the fetcher\n",
        "email_fetcher = EmailFetcher()\n",
        "print(\"‚úÖ EmailFetcher successfully created\")"
      ],
      "metadata": {
        "id": "6xr_lbsZrvJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2a: Enhanced Query Parser for Dynamic Email Retrieval\n",
        "\n",
        "class EmailQueryIntent(BaseModel):\n",
        "    \"\"\"Structured output for email query parsing.\"\"\"\n",
        "\n",
        "    email_count: Optional[int] = Field(\n",
        "        description=\"Number of emails to fetch (None means all matching emails)\"\n",
        "    )\n",
        "    time_filter: Optional[str] = Field(\n",
        "        description=\"Time-based filter (e.g., 'today', 'this_week', 'last_month')\"\n",
        "    )\n",
        "    status_filter: Optional[str] = Field(\n",
        "        description=\"Email status filter (e.g., 'unread', 'important', 'starred')\"\n",
        "    )\n",
        "    sender_filter: Optional[str] = Field(\n",
        "        description=\"Specific sender to filter by\"\n",
        "    )\n",
        "    subject_keywords: Optional[str] = Field(\n",
        "        description=\"Keywords to search in subject\"\n",
        "    )\n",
        "    label_filter: Optional[str] = Field(\n",
        "        description=\"Gmail label to filter by\"\n",
        "    )\n",
        "\n",
        "class IntelligentQueryParser:\n",
        "    \"\"\"\n",
        "    Parse user queries to extract email retrieval parameters dynamically.\n",
        "    Uses LLM for natural language understanding rather than rigid rules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_model: str = \"gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Initialise the query parser with LLM.\n",
        "\n",
        "        Args:\n",
        "            llm_model: Model to use for parsing\n",
        "        \"\"\"\n",
        "        self.llm = ChatOpenAI(model=llm_model, temperature=0.3)\n",
        "        self.output_parser = PydanticOutputParser(pydantic_object=EmailQueryIntent)\n",
        "\n",
        "    def parse_user_query(self, user_query: str) -> EmailQueryIntent:\n",
        "        \"\"\"\n",
        "        Parse user query to extract email retrieval parameters.\n",
        "\n",
        "        Args:\n",
        "            user_query: Natural language query from user\n",
        "\n",
        "        Returns:\n",
        "            EmailQueryIntent with parsed parameters\n",
        "        \"\"\"\n",
        "\n",
        "        format_instructions = self.output_parser.get_format_instructions()\n",
        "\n",
        "        prompt = f\"\"\"Parse the following email query to extract retrieval parameters.\n",
        "\n",
        "User Query: \"{user_query}\"\n",
        "\n",
        "Extract the following information:\n",
        "- email_count: If user specifies a number (e.g., \"5 emails\", \"last 10\"), extract it.\n",
        "  If they say \"all\", set to None. If no number mentioned, set to None.\n",
        "- time_filter: Extract time references like \"today\", \"yesterday\", \"this week\", \"last week\", \"this month\", \"last month\"\n",
        "  IMPORTANT: Preserve the exact format (e.g., \"this week\" not \"this_week\")\n",
        "- status_filter: Extract status like \"unread\", \"important\", \"starred\"\n",
        "  NOTE: If user says \"urgent\" or \"priority\", map it to \"important\"\n",
        "- sender_filter: Extract specific sender if mentioned (name or email)\n",
        "- subject_keywords: Extract any subject-related keywords or topics\n",
        "- label_filter: Extract Gmail labels if mentioned\n",
        "\n",
        "Examples:\n",
        "- \"Show me my last 5 unread emails\" ‚Üí email_count: 5, status_filter: \"unread\"\n",
        "- \"Get all emails from today\" ‚Üí email_count: None, time_filter: \"today\"\n",
        "- \"Fetch emails about the project\" ‚Üí subject_keywords: \"project\"\n",
        "- \"Show all unread emails\" ‚Üí email_count: None, status_filter: \"unread\"\n",
        "- \"Get all urgent emails from today\" ‚Üí email_count: None, time_filter: \"today\", status_filter: \"important\"\n",
        "- \"Find emails from this week about meetings\" ‚Üí time_filter: \"this week\", subject_keywords: \"meetings\"\n",
        "- \"Urgent messages from John\" ‚Üí status_filter: \"important\", sender_filter: \"John\"\n",
        "- \"Show me priority emails\" ‚Üí status_filter: \"important\"\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Parse the query and respond with the JSON object:\"\"\"\n",
        "\n",
        "        response = self.llm.invoke(prompt)\n",
        "\n",
        "        try:\n",
        "            # Parse the response\n",
        "            intent = self.output_parser.parse(response.content)\n",
        "            return intent\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to parse query intent: {e}\")\n",
        "            # Return default intent\n",
        "            return EmailQueryIntent()\n",
        "\n",
        "    def build_gmail_query(self, intent: EmailQueryIntent) -> tuple[str, int]:\n",
        "        \"\"\"\n",
        "        Build Gmail API query string from parsed intent.\n",
        "\n",
        "        Args:\n",
        "            intent: Parsed email query intent\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (gmail_query_string, max_results)\n",
        "        \"\"\"\n",
        "        query_parts = []\n",
        "\n",
        "        # Build query based on intent\n",
        "        if intent.status_filter:\n",
        "            # IMPROVEMENT 1: Map common terms to Gmail's actual filters\n",
        "            status_mappings = {\n",
        "                \"unread\": \"is:unread\",\n",
        "                \"important\": \"is:important\",\n",
        "                \"urgent\": \"is:important\",  # Map \"urgent\" to \"important\"\n",
        "                \"priority\": \"is:important\",  # Map \"priority\" to \"important\"\n",
        "                \"starred\": \"is:starred\",\n",
        "                \"read\": \"-is:unread\",  # Negative filter for read emails\n",
        "            }\n",
        "\n",
        "            filter_value = intent.status_filter.lower()\n",
        "            if filter_value in status_mappings:\n",
        "                query_parts.append(status_mappings[filter_value])\n",
        "            else:\n",
        "                # Try to use it as-is if not in mappings\n",
        "                query_parts.append(f\"is:{intent.status_filter}\")\n",
        "\n",
        "        if intent.time_filter:\n",
        "            # IMPROVEMENT 2: Handle both \"this week\" and \"this_week\" formats\n",
        "            time_mappings = {\n",
        "                \"today\": \"newer_than:1d\",\n",
        "                \"yesterday\": \"older_than:1d newer_than:2d\",\n",
        "                \"this week\": \"newer_than:7d\",  # Handle space version\n",
        "                \"this_week\": \"newer_than:7d\",  # Handle underscore version\n",
        "                \"last week\": \"older_than:7d newer_than:14d\",\n",
        "                \"last_week\": \"older_than:7d newer_than:14d\",\n",
        "                \"this month\": \"newer_than:30d\",\n",
        "                \"this_month\": \"newer_than:30d\",\n",
        "                \"last month\": \"older_than:30d newer_than:60d\",\n",
        "                \"last_month\": \"older_than:30d newer_than:60d\"\n",
        "            }\n",
        "\n",
        "            filter_value = intent.time_filter.lower()\n",
        "            if filter_value in time_mappings:\n",
        "                query_parts.append(time_mappings[filter_value])\n",
        "\n",
        "        if intent.sender_filter:\n",
        "            # IMPROVEMENT 3: Handle sender filter more intelligently\n",
        "            sender = intent.sender_filter.strip()\n",
        "            if '@' in sender:\n",
        "                query_parts.append(f\"from:{sender}\")\n",
        "            else:\n",
        "                # If no @, treat as name/partial match\n",
        "                query_parts.append(f\"from:{sender}\")\n",
        "\n",
        "        if intent.subject_keywords:\n",
        "            # IMPROVEMENT 4: Better handling of multi-word subjects\n",
        "            keywords = intent.subject_keywords.strip()\n",
        "            if ' ' in keywords:\n",
        "                # Multiple words - search for the phrase\n",
        "                query_parts.append(f'subject:\"{keywords}\"')\n",
        "            else:\n",
        "                # Single word\n",
        "                query_parts.append(f\"subject:{keywords}\")\n",
        "\n",
        "        if intent.label_filter:\n",
        "            label = intent.label_filter.strip()\n",
        "            query_parts.append(f\"label:{label}\")\n",
        "\n",
        "        # Default to inbox if no specific filters\n",
        "        if not query_parts:\n",
        "            query_parts.append(\"in:inbox\")\n",
        "\n",
        "        # Build the final query\n",
        "        gmail_query = \" \".join(query_parts)\n",
        "\n",
        "        # Determine max_results\n",
        "        # If email_count is None, fetch all (set a reasonable limit like 100)\n",
        "        # If specified, use that number\n",
        "        max_results = intent.email_count if intent.email_count else 100\n",
        "\n",
        "        return gmail_query, max_results\n",
        "\n",
        "# Enhanced fetch method for EmailFetcher\n",
        "def enhanced_fetch_emails_node(email_agent_instance, state):\n",
        "    \"\"\"\n",
        "    Enhanced email fetching with dynamic query parsing.\n",
        "    This replaces the rigid if-elif logic with intelligent parsing.\n",
        "    \"\"\"\n",
        "    print(\"üìß Intelligently parsing email request...\")\n",
        "\n",
        "    # Get the original user message\n",
        "    user_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
        "\n",
        "    # Initialise parser\n",
        "    parser = IntelligentQueryParser()\n",
        "\n",
        "    # Parse the query\n",
        "    intent = parser.parse_user_query(user_message)\n",
        "    print(f\"  Parsed intent: {intent.model_dump()}\")\n",
        "\n",
        "    # Build Gmail query\n",
        "    gmail_query, max_results = parser.build_gmail_query(intent)\n",
        "    print(f\"  Gmail query: '{gmail_query}' (max: {max_results})\")\n",
        "\n",
        "    try:\n",
        "        # Fetch emails using the parsed parameters\n",
        "        df = email_agent_instance.email_fetcher.fetch_emails(\n",
        "            query=gmail_query,\n",
        "            max_results=max_results\n",
        "        )\n",
        "\n",
        "        if not df.empty:\n",
        "            emails = df.to_dict('records')\n",
        "            state[\"email_data\"] = {\n",
        "                \"emails\": emails,\n",
        "                \"count\": len(emails),\n",
        "                \"query\": gmail_query,\n",
        "                \"intent\": intent.model_dump()\n",
        "            }\n",
        "            print(f\"  ‚úÖ Found {len(emails)} emails\")\n",
        "        else:\n",
        "            state[\"email_data\"] = {\n",
        "                \"emails\": [],\n",
        "                \"count\": 0,\n",
        "                \"query\": gmail_query,\n",
        "                \"intent\": intent.model_dump()\n",
        "            }\n",
        "            print(\"  ‚ö†Ô∏è No emails found matching criteria\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"email_data\"] = {\"error\": str(e)}\n",
        "        print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "print(\"‚úÖ Intelligent query parser created\")"
      ],
      "metadata": {
        "id": "8yK0FXgitzpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Security Analysis Module**"
      ],
      "metadata": {
        "id": "ogkvJZGBiUPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Security analysis module with hybrid LLM integration\n",
        "class SecurityAnalyser:\n",
        "    \"\"\"Email security analysis class with hybrid deterministic + LLM approach\"\"\"\n",
        "\n",
        "    def __init__(self, llm_model=None):\n",
        "        \"\"\"Initialise threat patterns and LLM integration\"\"\"\n",
        "\n",
        "        # LLM for domain assessment\n",
        "        self.security_llm = ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # Using lighter model for quick assessments\n",
        "            temperature=0.1,  # Low temperature for consistent security decisions\n",
        "        )\n",
        "\n",
        "        # Phishing/Scam patterns\n",
        "        self.phishing_patterns = [\n",
        "            # URL patterns\n",
        "            r'bit\\.ly|tinyurl|short\\.link|clck\\.ru',  # URL shorteners\n",
        "            r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}',  # IP addresses\n",
        "            r'@[^@]*@',  # Double @ signs\n",
        "\n",
        "            # Word patterns\n",
        "            r'urgent.{0,20}action.{0,20}required',\n",
        "            r'verify.{0,20}account.{0,20}immediately',\n",
        "            r'suspended.{0,20}account',\n",
        "            r'click.{0,20}here.{0,20}immediately',\n",
        "            r'limited.{0,20}time.{0,20}offer',\n",
        "            r'congratulations.{0,20}won',\n",
        "            r'claim.{0,20}prize',\n",
        "            r'tax.{0,20}refund',\n",
        "            r'nigerian?.{0,20}prince',\n",
        "            r'claim.{0,20}reward',\n",
        "        ]\n",
        "\n",
        "        # Prompt injection patterns\n",
        "        self.injection_patterns = [\n",
        "            r'ignore.{0,20}previous.{0,20}instructions',\n",
        "            r'disregard.{0,20}all.{0,20}prior',\n",
        "            r'forget.{0,20}everything',\n",
        "            r'new.{0,20}instructions.{0,20}follow',\n",
        "            r'system.{0,20}prompt.{0,20}override',\n",
        "            r'admin.{0,20}mode',\n",
        "            r'developer.{0,20}mode',\n",
        "            r'bypass.{0,20}security',\n",
        "            r'<script',  # XSS attempts\n",
        "            r'javascript:',\n",
        "            r'eval\\(',\n",
        "            r'onerror=',\n",
        "        ]\n",
        "\n",
        "    def assess_domain_with_llm(self, domain: str, email_context: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Use LLM to assess domain trustworthiness.\n",
        "        This method will be enhanced by DomainSimilarityMatcher if enabled.\n",
        "\n",
        "        Args:\n",
        "            domain: Domain to assess\n",
        "            email_context: Email context for better assessment\n",
        "\n",
        "        Returns:\n",
        "            Dict with assessment results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"Analyse this email sender domain for security risks.\n",
        "\n",
        "Domain: {domain}\n",
        "Email Subject: {email_context.get('subject', 'N/A')}\n",
        "Sender Full Address: {email_context.get('from', 'N/A')}\n",
        "\n",
        "Based on the domain name pattern and common phishing tactics, assess if this domain appears:\n",
        "1. SUSPICIOUS (likely phishing/scam)\n",
        "2. TRUSTED (legitimate business/service)\n",
        "3. UNKNOWN (cannot determine)\n",
        "\n",
        "Consider:\n",
        "- Does the domain mimic known brands?\n",
        "- Does it use suspicious patterns?\n",
        "- Is it a legitimate business domain?\n",
        "\n",
        "Respond with ONLY one word: SUSPICIOUS, TRUSTED, or UNKNOWN\n",
        "\n",
        "CRITICAL NOTE: Try not to mark as ‚ÄòUNKNOWN‚Äô as much as possible.\n",
        "\n",
        "Decision:\"\"\"\n",
        "\n",
        "            response = self.security_llm.invoke(prompt)\n",
        "            assessment = response.content.strip().upper()\n",
        "\n",
        "            # Ensure assessment is one of the expected values\n",
        "            if assessment not in ['SUSPICIOUS', 'TRUSTED', 'UNKNOWN']:\n",
        "                assessment = 'UNKNOWN'\n",
        "\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': assessment,\n",
        "                'confidence': 'low',  # Low confidence without similarity matching\n",
        "                'method': 'basic_llm'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è LLM assessment failed for {domain}: {e}\")\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': 'UNKNOWN',\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "    def check_phishing_indicators(self, email_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Check for phishing indicators with hybrid approach\"\"\"\n",
        "        indicators = []\n",
        "        risk_score = 0\n",
        "        domain_assessment = {}\n",
        "\n",
        "        # Analyse full text\n",
        "        text = f\"{email_data.get('subject', '')} {email_data.get('body_text', '')} {email_data.get('body_html', '')}\".lower()\n",
        "\n",
        "        # Pattern checking\n",
        "        for pattern in self.phishing_patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                indicators.append(f\"Suspicious pattern detected: {pattern}\")\n",
        "                risk_score += 20\n",
        "\n",
        "        # Urgency words analysis\n",
        "        urgency_words = ['urgent', 'immediate', 'expire', 'suspend', 'limited time']\n",
        "        urgency_count = sum(1 for word in urgency_words if word in text)\n",
        "        if urgency_count > 2:\n",
        "            indicators.append(f\"High urgency level detected ({urgency_count} keywords)\")\n",
        "            risk_score += urgency_count * 10\n",
        "\n",
        "        # Enhanced sender analysis\n",
        "        sender = email_data.get('from', '')\n",
        "        sender_domain = sender.split('@')[-1].split('>')[0] if '@' in sender else ''\n",
        "\n",
        "        if sender_domain:\n",
        "            # Check display name vs actual email\n",
        "            if '<' in sender and '>' in sender:\n",
        "                display_name = sender.split('<')[0].strip()\n",
        "                actual_email = sender.split('<')[1].split('>')[0]\n",
        "\n",
        "                # Check if display name contains different email\n",
        "                if '@' in display_name:\n",
        "                    indicators.append(\"Display name contains different email address\")\n",
        "                    risk_score += 30\n",
        "\n",
        "            # Domain assessment - now relies entirely on enhanced LLM method\n",
        "            # (which will use similarity matching if Cell 3a is loaded)\n",
        "            llm_result = self.assess_domain_with_llm(sender_domain, email_data)\n",
        "            domain_assessment = {\n",
        "                'status': llm_result['llm_assessment'].lower(),\n",
        "                'source': llm_result.get('method', 'llm'),\n",
        "                'confidence': llm_result.get('confidence', 'low')\n",
        "            }\n",
        "\n",
        "            if llm_result['llm_assessment'] == 'SUSPICIOUS':\n",
        "                indicators.append(f\"Domain assessed as suspicious: {sender_domain}\")\n",
        "                risk_score += 30\n",
        "\n",
        "            elif llm_result['llm_assessment'] == 'TRUSTED':\n",
        "                indicators.append(f\"Domain assessed as trusted: {sender_domain}\")\n",
        "                risk_score -= 10\n",
        "                risk_score = max(0, risk_score)\n",
        "\n",
        "            else:  # UNKNOWN\n",
        "                indicators.append(f\"Domain assessment inconclusive: {sender_domain}\")\n",
        "                risk_score += 10\n",
        "\n",
        "        # Attachment analysis\n",
        "        if email_data.get('has_attachment'):\n",
        "            attachments = email_data.get('attachment_names', [])\n",
        "            dangerous_extensions = ['.exe', '.zip', '.rar', '.bat', '.cmd', '.scr', '.vbs']\n",
        "\n",
        "            for att in attachments:\n",
        "                if any(att.lower().endswith(ext) for ext in dangerous_extensions):\n",
        "                    indicators.append(f\"Dangerous file extension detected: {att}\")\n",
        "                    risk_score += 40\n",
        "\n",
        "        return {\n",
        "            'indicators': indicators,\n",
        "            'risk_score': min(risk_score, 100),\n",
        "            'risk_level': self._calculate_risk_level(risk_score),\n",
        "            'domain_assessment': domain_assessment\n",
        "        }\n",
        "\n",
        "    def check_url_safety(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Check URL safety in email content\"\"\"\n",
        "        urls = re.findall(r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+', text)\n",
        "        suspicious_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            # Check for URL shorteners\n",
        "            if any(short in url.lower() for short in ['bit.ly', 'tinyurl', 'short.link']):\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'URL shortener detected - could hide malicious destination'\n",
        "                })\n",
        "\n",
        "            # Check for IP addresses instead of domains\n",
        "            if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url):\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'Contains IP address instead of domain name'\n",
        "                })\n",
        "\n",
        "            # Check for homograph attacks (similar looking characters)\n",
        "            if any(char in url for char in ['–∞', '–µ', '–æ', '—Ä', '—Å', '—É', '—Ö']):  # Cyrillic chars\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'Possible homograph attack - contains lookalike characters'\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'total_urls': len(urls),\n",
        "            'suspicious_urls': suspicious_urls,\n",
        "            'risk_level': 'high' if suspicious_urls else 'low'\n",
        "        }\n",
        "\n",
        "    def check_prompt_injection(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Check for prompt injection attempts\"\"\"\n",
        "        injections_found = []\n",
        "\n",
        "        for pattern in self.injection_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                injections_found.append({\n",
        "                    'pattern': pattern,\n",
        "                    'matches': matches[:3]  # First 3 matches\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'injection_detected': len(injections_found) > 0,\n",
        "            'injection_patterns': injections_found,\n",
        "            'risk_level': 'critical' if len(injections_found) > 2 else\n",
        "                         'high' if len(injections_found) > 0 else 'none'\n",
        "        }\n",
        "\n",
        "    def _calculate_risk_level(self, score: int) -> str:\n",
        "        \"\"\"Calculate risk level from score\"\"\"\n",
        "        if score >= 70:\n",
        "            return 'critical'\n",
        "        elif score >= 50:\n",
        "            return 'high'\n",
        "        elif score >= 30:\n",
        "            return 'medium'\n",
        "        elif score >= 10:\n",
        "            return 'low'\n",
        "        else:\n",
        "            return 'safe'\n",
        "\n",
        "    def analyse_email_security(self, email_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Complete security analysis with hybrid approach\"\"\"\n",
        "\n",
        "        # Run all analyses using FULL text\n",
        "        full_text = f\"{email_data.get('subject', '')} {email_data.get('body_text', '')} {email_data.get('body_html', '')}\"\n",
        "\n",
        "        url_analysis = self.check_url_safety(full_text)\n",
        "        phishing_analysis = self.check_phishing_indicators(email_data)\n",
        "        injection_analysis = self.check_prompt_injection(full_text)\n",
        "\n",
        "        # Calculate overall risk score\n",
        "        overall_risk_score = phishing_analysis['risk_score']\n",
        "\n",
        "        if url_analysis['risk_level'] == 'high':\n",
        "            overall_risk_score = min(overall_risk_score + 30, 100)\n",
        "\n",
        "        if injection_analysis['risk_level'] == 'critical':\n",
        "            overall_risk_score = min(overall_risk_score + 50, 100)\n",
        "        elif injection_analysis['risk_level'] == 'high':\n",
        "            overall_risk_score = min(overall_risk_score + 30, 100)\n",
        "\n",
        "        return {\n",
        "            'email_id': email_data.get('id'),\n",
        "            'subject': email_data.get('subject'),\n",
        "            'sender': email_data.get('from'),\n",
        "            'overall_risk_score': overall_risk_score,\n",
        "            'overall_risk_level': self._calculate_risk_level(overall_risk_score),\n",
        "            'domain_assessment': phishing_analysis.get('domain_assessment', {}),\n",
        "            'url_analysis': url_analysis,\n",
        "            'phishing_analysis': phishing_analysis,\n",
        "            'injection_analysis': injection_analysis,\n",
        "            'recommendations': self._generate_recommendations(\n",
        "                overall_risk_score,\n",
        "                phishing_analysis,\n",
        "                injection_analysis\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def _generate_recommendations(self, risk_score: int, phishing: Dict, injection: Dict) -> List[str]:\n",
        "        \"\"\"Generate security recommendations\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        if risk_score >= 70:\n",
        "            recommendations.append(\"‚õî CRITICAL: Do NOT open this email - DELETE immediately!\")\n",
        "            recommendations.append(\"üö® Report to IT security team\")\n",
        "        elif risk_score >= 50:\n",
        "            recommendations.append(\"‚ö†Ô∏è HIGH RISK: Do not click any links\")\n",
        "            recommendations.append(\"üîß Verify sender identity independently\")\n",
        "        elif risk_score >= 30:\n",
        "            recommendations.append(\"‚ö° CAUTION: Suspicious content detected\")\n",
        "\n",
        "        if injection['injection_detected']:\n",
        "            recommendations.append(\"ü§ñ PROMPT INJECTION detected - do not copy to AI systems\")\n",
        "\n",
        "        if phishing['indicators']:\n",
        "            recommendations.append(\"üé£ Phishing indicators detected - do not share personal information\")\n",
        "\n",
        "        # Add domain-specific recommendation\n",
        "        domain_assessment = phishing.get('domain_assessment', {})\n",
        "        if domain_assessment.get('confidence') == 'high' and domain_assessment.get('status') == 'suspicious':\n",
        "            recommendations.append(\"üîç AI assessment with similarity matching suggests sender domain is suspicious\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Test the analyser\n",
        "security_analyser = SecurityAnalyser()\n",
        "print(\"‚úÖ Cleaned SecurityAnalyser created - domain lists now managed in DomainSimilarityMatcher\")"
      ],
      "metadata": {
        "id": "Z2pBWoAqkxOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3a: Enhanced Domain Assessment with Similarity Matching\n",
        "class DomainSimilarityMatcher:\n",
        "    \"\"\"\n",
        "    Domain similarity matcher using vector embeddings for intelligent domain assessment.\n",
        "    Uses semantic similarity to find the most relevant examples for LLM decision-making.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings_model: str = \"text-embedding-3-small\"):\n",
        "        \"\"\"\n",
        "        Initialise the domain similarity matcher with vector stores.\n",
        "\n",
        "        Args:\n",
        "            embeddings_model: OpenAI embeddings model to use\n",
        "        \"\"\"\n",
        "        self.embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
        "\n",
        "        # Suspicious domains with context\n",
        "        self.suspicious_domains_data = [\n",
        "            (\"109.197.125.34.bc.googleusercontent.com\", \"IP address subdomain impersonating Google\"),\n",
        "            (\"accounts-mail.ru\", \"Russian domain mimicking account services\"),\n",
        "            (\"adobe-jkwefnewkjnfkjewnfkejwnfkjew.pages.dev\", \"Gibberish subdomain impersonating Adobe\"),\n",
        "            (\"business-facebook-covid19.com\", \"COVID-19 phishing using Facebook brand\"),\n",
        "            (\"google-secure.org\", \"Fake Google security domain\"),\n",
        "            (\"disceord.gift\", \"Discord typosquatting with gift scam\"),\n",
        "            (\"microsoft-error-pages-check-errors.pages.dev\", \"Fake Microsoft error page\"),\n",
        "            (\"steamcommunitc.com\", \"Steam typosquatting domain\"),\n",
        "            (\"xn--gmai-88b.com\", \"Homograph attack on Gmail\"),\n",
        "            (\"trust.twallet.cam\", \"Trust Wallet phishing domain\"),\n",
        "            # Add more as needed\n",
        "        ]\n",
        "\n",
        "        # Trusted domains with context\n",
        "        self.trusted_domains_data = [\n",
        "            (\"gmail.com\", \"Official Google email service\"),\n",
        "            (\"outlook.com\", \"Microsoft email service\"),\n",
        "            (\"amazon.com\", \"Official Amazon domain\"),\n",
        "            (\"github.com\", \"Official GitHub platform\"),\n",
        "            (\"linkedin.com\", \"Professional networking platform\"),\n",
        "            (\"paypal.com\", \"Official PayPal payment service\"),\n",
        "            (\"dropbox.com\", \"Cloud storage service\"),\n",
        "            (\"slack.com\", \"Team collaboration platform\"),\n",
        "            (\"zoom.us\", \"Video conferencing service\"),\n",
        "            (\"adobe.com\", \"Official Adobe domain\"),\n",
        "            # Add more as needed\n",
        "        ]\n",
        "\n",
        "        # Create vector stores\n",
        "        self._initialise_vector_stores()\n",
        "\n",
        "    def _initialise_vector_stores(self):\n",
        "        \"\"\"Create FAISS vector stores for domain similarity search.\"\"\"\n",
        "\n",
        "        # Create documents for suspicious domains\n",
        "        suspicious_docs = [\n",
        "            Document(\n",
        "                page_content=f\"{domain} - {context}\",\n",
        "                metadata={\"domain\": domain, \"type\": \"suspicious\", \"context\": context}\n",
        "            )\n",
        "            for domain, context in self.suspicious_domains_data\n",
        "        ]\n",
        "\n",
        "        # Create documents for trusted domains\n",
        "        trusted_docs = [\n",
        "            Document(\n",
        "                page_content=f\"{domain} - {context}\",\n",
        "                metadata={\"domain\": domain, \"type\": \"trusted\", \"context\": context}\n",
        "            )\n",
        "            for domain, context in self.trusted_domains_data\n",
        "        ]\n",
        "\n",
        "        # Create FAISS vector stores\n",
        "        self.suspicious_vectorstore = FAISS.from_documents(\n",
        "            suspicious_docs, self.embeddings\n",
        "        )\n",
        "        self.trusted_vectorstore = FAISS.from_documents(\n",
        "            trusted_docs, self.embeddings\n",
        "        )\n",
        "\n",
        "    def get_similar_domains(\n",
        "        self,\n",
        "        query_domain: str,\n",
        "        top_k: int = 5\n",
        "    ) -> Tuple[List[Document], List[Document]]:\n",
        "        \"\"\"\n",
        "        Retrieve the most similar suspicious and trusted domains.\n",
        "\n",
        "        Args:\n",
        "            query_domain: Domain to assess\n",
        "            top_k: Number of similar examples to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (suspicious_examples, trusted_examples)\n",
        "        \"\"\"\n",
        "        # Search for similar suspicious domains\n",
        "        suspicious_similar = self.suspicious_vectorstore.similarity_search(\n",
        "            query_domain, k=top_k\n",
        "        )\n",
        "\n",
        "        # Search for similar trusted domains\n",
        "        trusted_similar = self.trusted_vectorstore.similarity_search(\n",
        "            query_domain, k=top_k\n",
        "        )\n",
        "\n",
        "        return suspicious_similar, trusted_similar\n",
        "\n",
        "    def create_assessment_prompt(\n",
        "        self,\n",
        "        domain: str,\n",
        "        email_context: dict,\n",
        "        top_k: int = 5\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Create an enhanced prompt with similar domain examples.\n",
        "\n",
        "        Args:\n",
        "            domain: Domain to assess\n",
        "            email_context: Email context for assessment\n",
        "            top_k: Number of examples to include\n",
        "\n",
        "        Returns:\n",
        "            Enhanced prompt with examples\n",
        "        \"\"\"\n",
        "        # Get similar domains\n",
        "        suspicious_examples, trusted_examples = self.get_similar_domains(domain, top_k)\n",
        "\n",
        "        # Format examples\n",
        "        suspicious_text = \"\\n\".join([\n",
        "            f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\"\n",
        "            for doc in suspicious_examples\n",
        "        ])\n",
        "\n",
        "        trusted_text = \"\\n\".join([\n",
        "            f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\"\n",
        "            for doc in trusted_examples\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"Analyse this email sender domain for security risks using the provided examples.\n",
        "\n",
        "Domain to assess: {domain}\n",
        "Email Subject: {email_context.get('subject', 'N/A')}\n",
        "Sender Full Address: {email_context.get('from', 'N/A')}\n",
        "\n",
        "SIMILAR SUSPICIOUS DOMAINS (for reference):\n",
        "{suspicious_text}\n",
        "\n",
        "SIMILAR TRUSTED DOMAINS (for reference):\n",
        "{trusted_text}\n",
        "\n",
        "Based on:\n",
        "1. The similarity to the suspicious examples above\n",
        "2. The similarity to the trusted examples above\n",
        "3. Common phishing patterns (typosquatting, brand impersonation, etc.)\n",
        "4. The email context provided\n",
        "\n",
        "Assess if this domain appears:\n",
        "- SUSPICIOUS (likely phishing/scam)\n",
        "- TRUSTED (legitimate business/service)\n",
        "- UNKNOWN (cannot determine with confidence)\n",
        "\n",
        "Consider:\n",
        "- Does it closely resemble any suspicious examples?\n",
        "- Does it match patterns from trusted examples?\n",
        "- Are there spelling variations of known brands?\n",
        "- Does it use suspicious subdomain patterns?\n",
        "\n",
        "Respond with ONLY one word: SUSPICIOUS, TRUSTED, or UNKNOWN\n",
        "\n",
        "Decision:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "# Integration with SecurityAnalyser\n",
        "def integrate_similarity_matcher(security_analyser_class):\n",
        "    \"\"\"\n",
        "    Monkey-patch the SecurityAnalyser to use similarity matching.\n",
        "    In production, this should be properly integrated into the class.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise the matcher\n",
        "    domain_matcher = DomainSimilarityMatcher()\n",
        "\n",
        "    # Store original method\n",
        "    original_assess = security_analyser_class.assess_domain_with_llm\n",
        "\n",
        "    def enhanced_assess_domain(self, domain: str, email_context: dict) -> dict:\n",
        "        \"\"\"Enhanced domain assessment using similarity matching.\"\"\"\n",
        "        try:\n",
        "            # Create enhanced prompt with examples\n",
        "            prompt = domain_matcher.create_assessment_prompt(\n",
        "                domain, email_context, top_k=5\n",
        "            )\n",
        "\n",
        "            # Get LLM assessment\n",
        "            response = self.security_llm.invoke(prompt)\n",
        "            assessment = response.content.strip().upper()\n",
        "\n",
        "            # Validate response\n",
        "            if assessment not in ['SUSPICIOUS', 'TRUSTED', 'UNKNOWN']:\n",
        "                assessment = 'UNKNOWN'\n",
        "\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': assessment,\n",
        "                'confidence': 'high' if assessment != 'UNKNOWN' else 'medium',\n",
        "                'method': 'similarity_enhanced'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced assessment failed for {domain}: {e}\")\n",
        "            # Fall back to original method\n",
        "            return original_assess(self, domain, email_context)\n",
        "\n",
        "    # Replace method\n",
        "    security_analyser_class.assess_domain_with_llm = enhanced_assess_domain\n",
        "\n",
        "    return domain_matcher\n",
        "\n",
        "print(\"‚úÖ Domain similarity matcher created\")"
      ],
      "metadata": {
        "id": "gnp4E4K0tzuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain Agent Tools**"
      ],
      "metadata": {
        "id": "zCFGmafDlHEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Enhanced Tools with @tool Decorator and Full Content Processing\n",
        "# Define input schemas for tools\n",
        "class EmailSummaryInput(BaseModel):\n",
        "    \"\"\"Input schema for email summarisation.\"\"\"\n",
        "    email_id: str = Field(description=\"Email ID to summarise\")\n",
        "    subject: str = Field(description=\"Email subject\")\n",
        "    sender: str = Field(description=\"Email sender\")\n",
        "    body_text: str = Field(description=\"Full email body text\")\n",
        "    body_html: Optional[str] = Field(description=\"Full email HTML content if available\")\n",
        "\n",
        "class EmailActionInput(BaseModel):\n",
        "    \"\"\"Input schema for action extraction.\"\"\"\n",
        "    email_id: str = Field(description=\"Email ID\")\n",
        "    subject: str = Field(description=\"Email subject\")\n",
        "    body_text: str = Field(description=\"Full email body text\")\n",
        "    sender: str = Field(description=\"Email sender\")\n",
        "\n",
        "class EmailSecurityInput(BaseModel):\n",
        "    \"\"\"Input schema for security analysis.\"\"\"\n",
        "    email_data: Dict[str, Any] = Field(description=\"Complete email data dictionary\")\n",
        "\n",
        "# Create LLM for summarisation (separate from security)\n",
        "summarisation_llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.3  # Slightly higher for more natural summaries\n",
        ")\n",
        "\n",
        "@tool(\"email_summariser\", args_schema=EmailSummaryInput, return_direct=False)\n",
        "def summarise_email(\n",
        "    email_id: str,\n",
        "    subject: str,\n",
        "    sender: str,\n",
        "    body_text: str,\n",
        "    body_html: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Provide comprehensive email summarisation using full content.\n",
        "    This tool reads the ENTIRE email content to create proper summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use full body text, not regex extracts\n",
        "    full_content = body_text if body_text else \"\"\n",
        "\n",
        "    # If HTML is available and text is empty, extract text from HTML\n",
        "    if not full_content and body_html:\n",
        "        # Basic HTML tag removal (in production, use BeautifulSoup)\n",
        "        import re\n",
        "        full_content = re.sub('<[^<]+?>', '', body_html)\n",
        "\n",
        "    # Create comprehensive summarisation prompt\n",
        "    prompt = f\"\"\"Provide a comprehensive summary of this email.\n",
        "\n",
        "Email Details:\n",
        "- Subject: {subject}\n",
        "- From: {sender}\n",
        "- Email ID: {email_id}\n",
        "\n",
        "Full Email Content:\n",
        "{full_content[:5000]}  # Limit to 5000 chars for token management\n",
        "\n",
        "Create a detailed summary that includes:\n",
        "1. **Main Purpose**: What is the primary reason for this email?\n",
        "2. **Key Points**: List the most important information (3-5 points)\n",
        "3. **Context**: Any relevant background or context mentioned\n",
        "4. **Tone**: Professional, casual, urgent, informative, etc.\n",
        "5. **Important Details**: Dates, numbers, names, specific requirements\n",
        "6. **Attachments Mentioned**: Any files or documents referenced\n",
        "\n",
        "Provide a clear, comprehensive summary that would allow someone to understand\n",
        "the email without reading it. Focus on clarity and completeness.\n",
        "\n",
        "Format your response as JSON with the following structure:\n",
        "{{\n",
        "    \"main_purpose\": \"...\",\n",
        "    \"key_points\": [\"point1\", \"point2\", ...],\n",
        "    \"context\": \"...\",\n",
        "    \"tone\": \"...\",\n",
        "    \"important_details\": {{\n",
        "        \"dates\": [...],\n",
        "        \"numbers\": [...],\n",
        "        \"names\": [...],\n",
        "        \"requirements\": [...]\n",
        "    }},\n",
        "    \"attachments_mentioned\": [...],\n",
        "    \"executive_summary\": \"A 2-3 sentence overview\",\n",
        "    \"word_count\": <approximate word count of original email>\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = summarisation_llm.invoke(prompt)\n",
        "\n",
        "        # Parse JSON response\n",
        "        content = response.content\n",
        "        # Clean potential markdown formatting\n",
        "        if \"```json\" in content:\n",
        "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "        elif \"```\" in content:\n",
        "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "\n",
        "        summary_data = json.loads(content.strip())\n",
        "\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"sender\": sender,\n",
        "            \"summary\": summary_data,\n",
        "            \"content_length\": len(full_content),\n",
        "            \"summarisation_successful\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Summarisation failed: {e}\")\n",
        "        # Fallback to basic summary\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"sender\": sender,\n",
        "            \"summary\": {\n",
        "                \"executive_summary\": f\"Email from {sender} about: {subject}\",\n",
        "                \"error\": str(e)\n",
        "            },\n",
        "            \"content_length\": len(full_content),\n",
        "            \"summarisation_successful\": False\n",
        "        }\n",
        "\n",
        "@tool(\"extract_actions\", args_schema=EmailActionInput, return_direct=False)\n",
        "def extract_action_items(\n",
        "    email_id: str,\n",
        "    subject: str,\n",
        "    body_text: str,\n",
        "    sender: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract action items and to-dos from email using full content analysis.\n",
        "    Uses LLM to understand context and extract meaningful actions.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Extract all action items, tasks, and deadlines from this email.\n",
        "\n",
        "Email Subject: {subject}\n",
        "From: {sender}\n",
        "\n",
        "Full Email Content:\n",
        "{body_text[:5000]}\n",
        "\n",
        "Identify and extract:\n",
        "1. **Direct Requests**: Things explicitly asked to be done\n",
        "2. **Implied Tasks**: Actions implied but not directly stated\n",
        "3. **Deadlines**: Any time-sensitive items with dates\n",
        "4. **Follow-ups**: Items requiring response or follow-up\n",
        "5. **Decisions Required**: Points needing decisions\n",
        "6. **Information Requests**: Requests for information or documents\n",
        "\n",
        "For each action item, specify:\n",
        "- The specific action required\n",
        "- Who needs to do it (if mentioned)\n",
        "- Deadline or timeframe (if any)\n",
        "- Priority level (High/Medium/Low based on context)\n",
        "- Category (Request/Deadline/Follow-up/Decision/Information)\n",
        "\n",
        "Format as JSON:\n",
        "{{\n",
        "    \"action_items\": [\n",
        "        {{\n",
        "            \"action\": \"...\",\n",
        "            \"assigned_to\": \"recipient/specific person/not specified\",\n",
        "            \"deadline\": \"date or null\",\n",
        "            \"priority\": \"High/Medium/Low\",\n",
        "            \"category\": \"...\"\n",
        "        }}\n",
        "    ],\n",
        "    \"total_actions\": <number>,\n",
        "    \"has_urgent_items\": true/false,\n",
        "    \"summary_of_requirements\": \"Brief overview of what's needed\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = summarisation_llm.invoke(prompt)\n",
        "\n",
        "        # Parse response\n",
        "        content = response.content\n",
        "        if \"```json\" in content:\n",
        "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "        elif \"```\" in content:\n",
        "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "\n",
        "        actions_data = json.loads(content.strip())\n",
        "\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"actions\": actions_data,\n",
        "            \"extraction_successful\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Action extraction failed: {e}\")\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"actions\": {\n",
        "                \"action_items\": [],\n",
        "                \"total_actions\": 0,\n",
        "                \"error\": str(e)\n",
        "            },\n",
        "            \"extraction_successful\": False\n",
        "        }\n",
        "\n",
        "@tool(\"security_analysis\", args_schema=EmailSecurityInput, return_direct=False)\n",
        "def analyse_email_security(email_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Perform comprehensive security analysis on email.\n",
        "    Maintains existing security logic but as a proper tool.\n",
        "    \"\"\"\n",
        "\n",
        "    # This would use the existing SecurityAnalyser class\n",
        "    # but wrapped as a proper tool\n",
        "    from typing import TYPE_CHECKING\n",
        "    if not TYPE_CHECKING:\n",
        "        # Import would happen here in production\n",
        "        pass\n",
        "\n",
        "    # For now, return a structured response\n",
        "    # In production, this would call the actual SecurityAnalyser\n",
        "\n",
        "    security_result = {\n",
        "        \"email_id\": email_data.get(\"id\"),\n",
        "        \"risk_assessment\": {\n",
        "            \"overall_risk\": \"low\",  # This would be calculated\n",
        "            \"risk_score\": 0,\n",
        "            \"risk_factors\": []\n",
        "        },\n",
        "        \"recommendations\": [],\n",
        "        \"security_check_complete\": True\n",
        "    }\n",
        "\n",
        "    return security_result\n",
        "\n",
        "# Tool registration function for LangChain\n",
        "def get_enhanced_tools():\n",
        "    \"\"\"\n",
        "    Get all enhanced tools with proper structure.\n",
        "    These tools will show up correctly in LangSmith traces.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        summarise_email,\n",
        "        extract_action_items,\n",
        "        analyse_email_security\n",
        "    ]\n",
        "\n",
        "print(\"‚úÖ Enhanced tools with @tool decorator created\")"
      ],
      "metadata": {
        "id": "3YDeZMWIv88d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main Agent Configuration**"
      ],
      "metadata": {
        "id": "zPBZA03yTQAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Enhanced Email Agent with Proper Integration\n",
        "class EnhancedAgentState(TypedDict):\n",
        "    \"\"\"Enhanced state for the email agent.\"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    user_query: str\n",
        "    parsed_intent: Dict[str, Any]\n",
        "    email_data: Dict[str, Any]\n",
        "    summaries: List[Dict[str, Any]]\n",
        "    actions: List[Dict[str, Any]]\n",
        "    security_results: List[Dict[str, Any]]\n",
        "    final_output: str\n",
        "\n",
        "class EnhancedEmailAgent:\n",
        "    \"\"\"\n",
        "    Enhanced email management agent with proper tool integration,\n",
        "    dynamic query parsing, and comprehensive summarisation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"gpt-4o-mini\",\n",
        "        temperature: float = 0.3,\n",
        "        use_similarity_matching: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the enhanced agent.\n",
        "\n",
        "        Args:\n",
        "            model_name: LLM model to use\n",
        "            temperature: Model temperature\n",
        "            use_similarity_matching: Whether to use domain similarity matching\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialise LLM\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
        "\n",
        "        # Initialise components\n",
        "        self.email_fetcher = EmailFetcher()\n",
        "        self.query_parser = IntelligentQueryParser()\n",
        "\n",
        "        # Initialise enhanced security analyser\n",
        "        self.security_analyser = SecurityAnalyser()\n",
        "        if use_similarity_matching:\n",
        "            self.domain_matcher = integrate_similarity_matcher(SecurityAnalyser)\n",
        "\n",
        "        # Get enhanced tools\n",
        "        self.tools = get_enhanced_tools()\n",
        "\n",
        "        # Build the enhanced graph\n",
        "        self.app = self._build_enhanced_graph()\n",
        "\n",
        "    def _build_enhanced_graph(self) -> StateGraph:\n",
        "        \"\"\"Build the enhanced LangGraph workflow.\"\"\"\n",
        "\n",
        "        # Create workflow\n",
        "        workflow = StateGraph(EnhancedAgentState)\n",
        "\n",
        "        # Add nodes\n",
        "        workflow.add_node(\"parse_query\", self._parse_query)\n",
        "        workflow.add_node(\"fetch_emails\", self._fetch_emails)\n",
        "        workflow.add_node(\"security_check\", self._security_analysis)\n",
        "        workflow.add_node(\"summarise_emails\", self._summarise_emails)\n",
        "        workflow.add_node(\"extract_actions\", self._extract_actions)\n",
        "        workflow.add_node(\"generate_output\", self._generate_final_output)\n",
        "\n",
        "        # Define the flow\n",
        "        workflow.set_entry_point(\"parse_query\")\n",
        "        workflow.add_edge(\"parse_query\", \"fetch_emails\")\n",
        "        workflow.add_edge(\"fetch_emails\", \"security_check\")\n",
        "        workflow.add_edge(\"security_check\", \"summarise_emails\")\n",
        "        workflow.add_edge(\"summarise_emails\", \"extract_actions\")\n",
        "        workflow.add_edge(\"extract_actions\", \"generate_output\")\n",
        "        workflow.add_edge(\"generate_output\", END)\n",
        "\n",
        "        # Compile\n",
        "        return workflow.compile()\n",
        "\n",
        "    def _parse_query(self, state: EnhancedAgentState) -> EnhancedAgentState:\n",
        "        \"\"\"Parse user query to understand intent.\"\"\"\n",
        "\n",
        "        print(\"üß† Parsing user query with intelligence...\")\n",
        "\n",
        "        user_query = state.get(\"user_query\", \"\")\n",
        "\n",
        "        # Parse the query\n",
        "        intent = self.query_parser.parse_user_query(user_query)\n",
        "        state[\"parsed_intent\"] = intent.model_dump()\n",
        "\n",
        "        print(f\"  ‚úÖ Intent parsed: {intent.model_dump()}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _fetch_emails(self, state: EnhancedAgentState) -> EnhancedAgentState:\n",
        "        \"\"\"Fetch emails based on parsed intent.\"\"\"\n",
        "\n",
        "        print(\"üìß Fetching emails based on parsed intent...\")\n",
        "\n",
        "        intent = EmailQueryIntent(**state[\"parsed_intent\"])\n",
        "\n",
        "        # Build Gmail query\n",
        "        gmail_query, max_results = self.query_parser.build_gmail_query(intent)\n",
        "\n",
        "        print(f\"  Query: '{gmail_query}' (max: {max_results})\")\n",
        "\n",
        "        try:\n",
        "            df = self.email_fetcher.fetch_emails(gmail_query, max_results)\n",
        "\n",
        "            if not df.empty:\n",
        "                emails = df.to_dict('records')\n",
        "                state[\"email_data\"] = {\n",
        "                    \"emails\": emails,\n",
        "                    \"count\": len(emails),\n",
        "                    \"query\": gmail_query\n",
        "                }\n",
        "                print(f\"  ‚úÖ Fetched {len(emails)} emails\")\n",
        "            else:\n",
        "                state[\"email_data\"] = {\"emails\": [], \"count\": 0}\n",
        "                print(\"  ‚ö†Ô∏è No emails found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            state[\"email_data\"] = {\"error\": str(e)}\n",
        "            print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _security_analysis(self, state: EnhancedAgentState) -> EnhancedAgentState:\n",
        "        \"\"\"Perform security analysis on all emails.\"\"\"\n",
        "\n",
        "        print(\"üîí Performing security analysis...\")\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "        security_results = []\n",
        "\n",
        "        for email in emails:\n",
        "            # Use the enhanced security analyser\n",
        "            result = self.security_analyser.analyse_email_security(email)\n",
        "            security_results.append(result)\n",
        "\n",
        "        state[\"security_results\"] = security_results\n",
        "\n",
        "        high_risk = sum(1 for r in security_results\n",
        "                       if r.get(\"overall_risk_level\") in [\"high\", \"critical\"])\n",
        "        print(f\"  ‚úÖ Security check complete: {high_risk} high-risk emails\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _summarise_emails(self, state: EnhancedAgentState) -> EnhancedAgentState:\n",
        "        \"\"\"Create comprehensive summaries using full email content.\"\"\"\n",
        "\n",
        "        print(\"üìù Creating comprehensive summaries...\")\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "        summaries = []\n",
        "\n",
        "        for email in emails:\n",
        "            # Use the enhanced summarisation tool with FULL content\n",
        "            summary = summarise_email.invoke({\n",
        "                \"email_id\": email.get(\"id\"),\n",
        "                \"subject\": email.get(\"subject\"),\n",
        "                \"sender\": email.get(\"from\"),\n",
        "                \"body_text\": email.get(\"body_text\", \"\"),  # FULL text\n",
        "                \"body_html\": email.get(\"body_html\", \"\")   # FULL HTML\n",
        "            })\n",
        "            summaries.append(summary)\n",
        "\n",
        "        state[\"summaries\"] = summaries\n",
        "        print(f\"  ‚úÖ Created {len(summaries)} comprehensive summaries\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _extract_actions(self, state: EnhancedAgentState) -> EnhancedAgentState:\n",
        "        \"\"\"Extract action items from emails.\"\"\"\n",
        "\n",
        "        print(\"üìã Extracting action items...\")\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "        actions = []\n",
        "\n",
        "        for email in emails:\n",
        "            # Use the enhanced action extraction tool\n",
        "            action_result = extract_action_items.invoke({\n",
        "                \"email_id\": email.get(\"id\"),\n",
        "                \"subject\": email.get(\"subject\"),\n",
        "                \"body_text\": email.get(\"body_text\", \"\"),  # FULL text\n",
        "                \"sender\": email.get(\"from\")\n",
        "            })\n",
        "            actions.append(action_result)\n",
        "\n",
        "        state[\"actions\"] = actions\n",
        "\n",
        "        total_actions = sum(\n",
        "            a.get(\"actions\", {}).get(\"total_actions\", 0)\n",
        "            for a in actions\n",
        "        )\n",
        "        print(f\"  ‚úÖ Extracted {total_actions} total action items\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _generate_final_output(self, state: EnhancedAgentState) -> EnhancedAgentState:\n",
        "        \"\"\"Generate comprehensive final output with proper structure.\"\"\"\n",
        "\n",
        "        print(\"üìä Generating comprehensive final report...\")\n",
        "\n",
        "        # Build structured output\n",
        "        output = self._build_comprehensive_output(state)\n",
        "\n",
        "        state[\"final_output\"] = output\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _build_comprehensive_output(self, state: EnhancedAgentState) -> str:\n",
        "        \"\"\"\n",
        "        Build comprehensive output with:\n",
        "        1. Security warnings (if any)\n",
        "        2. Full email summaries\n",
        "        3. Consolidated action items\n",
        "        4. Clear structure and formatting\n",
        "        \"\"\"\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "        summaries = state.get(\"summaries\", [])\n",
        "        actions = state.get(\"actions\", [])\n",
        "        security_results = state.get(\"security_results\", [])\n",
        "\n",
        "        output = \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += \"üìä COMPREHENSIVE EMAIL ANALYSIS REPORT\\n\"\n",
        "        output += \"=\"*80 + \"\\n\\n\"\n",
        "\n",
        "        # Overview\n",
        "        output += f\"üìß Total Emails Analysed: {len(emails)}\\n\"\n",
        "        output += f\"üîç Query Used: {state.get('email_data', {}).get('query', 'N/A')}\\n\"\n",
        "\n",
        "        # Security Section First (Priority)\n",
        "        high_risk_emails = [\n",
        "            (i, r) for i, r in enumerate(security_results)\n",
        "            if r.get(\"overall_risk_level\") in [\"high\", \"critical\"]\n",
        "        ]\n",
        "\n",
        "        if high_risk_emails:\n",
        "            output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "            output += \"‚ö†Ô∏è SECURITY ALERTS - IMMEDIATE ATTENTION REQUIRED\\n\"\n",
        "            output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "            for idx, risk in high_risk_emails:\n",
        "                email = emails[idx]\n",
        "                output += f\"\\nüî¥ HIGH RISK: {email.get('subject', 'No subject')}\\n\"\n",
        "                output += f\"   From: {email.get('from', 'Unknown')}\\n\"\n",
        "                output += f\"   Risk Level: {risk.get('overall_risk_level', '').upper()}\\n\"\n",
        "                output += f\"   Risk Score: {risk.get('overall_risk_score', 0)}/100\\n\"\n",
        "\n",
        "                recommendations = risk.get(\"recommendations\", [])\n",
        "                if recommendations:\n",
        "                    output += \"   Recommendations:\\n\"\n",
        "                    for rec in recommendations[:3]:\n",
        "                        output += f\"     ‚Ä¢ {rec}\\n\"\n",
        "\n",
        "        # Full Email Summaries Section\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += \"üìù DETAILED EMAIL SUMMARIES\\n\"\n",
        "        output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "        for i, (email, summary, action, security) in enumerate(\n",
        "            zip(emails, summaries, actions, security_results), 1\n",
        "        ):\n",
        "            output += f\"\\n{i}. {email.get('subject', 'No subject')}\\n\"\n",
        "            output += \"-\"*60 + \"\\n\"\n",
        "            output += f\"From: {email.get('from', 'Unknown')}\\n\"\n",
        "            output += f\"Date: {str(email.get('date', 'Unknown'))[:19]}\\n\"\n",
        "            output += f\"Security Status: {security.get('overall_risk_level', 'unchecked')}\\n\"\n",
        "\n",
        "            # Comprehensive Summary\n",
        "            if summary.get(\"summarisation_successful\"):\n",
        "                sum_data = summary.get(\"summary\", {})\n",
        "                output += f\"\\nüìå Executive Summary:\\n\"\n",
        "                output += f\"   {sum_data.get('executive_summary', 'Not available')}\\n\"\n",
        "\n",
        "                output += f\"\\nüìç Main Purpose:\\n\"\n",
        "                output += f\"   {sum_data.get('main_purpose', 'Not identified')}\\n\"\n",
        "\n",
        "                key_points = sum_data.get(\"key_points\", [])\n",
        "                if key_points:\n",
        "                    output += f\"\\nüîë Key Points:\\n\"\n",
        "                    for point in key_points:\n",
        "                        output += f\"   ‚Ä¢ {point}\\n\"\n",
        "\n",
        "                important = sum_data.get(\"important_details\", {})\n",
        "                if any(important.values()):\n",
        "                    output += f\"\\nüìä Important Details:\\n\"\n",
        "                    if important.get(\"dates\"):\n",
        "                        output += f\"   Dates: {', '.join(important['dates'])}\\n\"\n",
        "                    if important.get(\"requirements\"):\n",
        "                        output += f\"   Requirements: {', '.join(important['requirements'])}\\n\"\n",
        "            else:\n",
        "                output += f\"\\n‚ö†Ô∏è Summarisation failed: {summary.get('summary', {}).get('error', 'Unknown error')}\\n\"\n",
        "\n",
        "        # Consolidated Action Items\n",
        "        all_actions = []\n",
        "        for i, action_set in enumerate(actions):\n",
        "            if action_set.get(\"extraction_successful\"):\n",
        "                for action in action_set.get(\"actions\", {}).get(\"action_items\", []):\n",
        "                    action[\"email_subject\"] = emails[i].get(\"subject\", \"\")\n",
        "                    all_actions.append(action)\n",
        "\n",
        "        if all_actions:\n",
        "            output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "            output += \"üìã CONSOLIDATED ACTION ITEMS & TO-DO LIST\\n\"\n",
        "            output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "            # Sort by priority\n",
        "            high_priority = [a for a in all_actions if a.get(\"priority\") == \"High\"]\n",
        "            medium_priority = [a for a in all_actions if a.get(\"priority\") == \"Medium\"]\n",
        "            low_priority = [a for a in all_actions if a.get(\"priority\") == \"Low\"]\n",
        "\n",
        "            if high_priority:\n",
        "                output += \"\\nüî¥ HIGH PRIORITY:\\n\"\n",
        "                for action in high_priority:\n",
        "                    output += f\"   ‚ñ° {action['action']}\\n\"\n",
        "                    if action.get(\"deadline\"):\n",
        "                        output += f\"     ‚è∞ Deadline: {action['deadline']}\\n\"\n",
        "                    output += f\"     üìß From: {action['email_subject'][:50]}...\\n\"\n",
        "\n",
        "            if medium_priority:\n",
        "                output += \"\\nüü° MEDIUM PRIORITY:\\n\"\n",
        "                for action in medium_priority:\n",
        "                    output += f\"   ‚ñ° {action['action']}\\n\"\n",
        "                    if action.get(\"deadline\"):\n",
        "                        output += f\"     ‚è∞ Deadline: {action['deadline']}\\n\"\n",
        "\n",
        "            if low_priority:\n",
        "                output += \"\\nüü¢ LOW PRIORITY:\\n\"\n",
        "                for action in low_priority[:5]:  # Limit to 5\n",
        "                    output += f\"   ‚ñ° {action['action']}\\n\"\n",
        "\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += \"‚úÖ END OF REPORT\\n\"\n",
        "        output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def process_emails(self, user_query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process user email query with enhanced capabilities.\n",
        "\n",
        "        Args:\n",
        "            user_query: Natural language query from user\n",
        "\n",
        "        Returns:\n",
        "            Dict with results and status\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ü§ñ ENHANCED EMAIL AGENT ACTIVE\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüìù User Query: {user_query}\\n\")\n",
        "\n",
        "        try:\n",
        "            # Initial state\n",
        "            initial_state = {\n",
        "                \"messages\": [HumanMessage(content=user_query)],\n",
        "                \"user_query\": user_query,\n",
        "                \"parsed_intent\": {},\n",
        "                \"email_data\": {},\n",
        "                \"summaries\": [],\n",
        "                \"actions\": [],\n",
        "                \"security_results\": [],\n",
        "                \"final_output\": \"\"\n",
        "            }\n",
        "\n",
        "            # Run the workflow\n",
        "            final_state = self.app.invoke(initial_state)\n",
        "\n",
        "            # Display result\n",
        "            print(final_state.get(\"final_output\", \"No result\"))\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'output': final_state.get(\"final_output\"),\n",
        "                'state': final_state\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "print(\"‚úÖ Enhanced Email Agent created with all improvements\")"
      ],
      "metadata": {
        "id": "uA-W8Q-OwN95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialise the Enhanced Agent**"
      ],
      "metadata": {
        "id": "pzhAQtI56rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After all cells are loaded\n",
        "enhanced_agent = EnhancedEmailAgent(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.3,\n",
        "    use_similarity_matching=True  # Enable domain similarity\n",
        ")\n",
        "\n",
        "# Test with various queries\n",
        "result = enhanced_agent.process_emails(\"Show me my last 5 unread emails\")\n",
        "result = enhanced_agent.process_emails(\"Get all emails from today\")\n",
        "result = enhanced_agent.process_emails(\"Summarise last 5 important emails\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AO6Z_fK76rHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test**"
      ],
      "metadata": {
        "id": "koTjdqgA-Xbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'enhanced_agent' in locals():\n",
        "    # The domain_matcher is stored as an attribute of the agent\n",
        "    domain_matcher = enhanced_agent.domain_matcher\n",
        "\n",
        "    # Now you can test it\n",
        "    test_domain = \"arnazon.com\"  # Note the typo\n",
        "    suspicious, trusted = domain_matcher.get_similar_domains(test_domain)\n",
        "\n",
        "    print(\"‚úÖ Domain Matcher Test Results:\")\n",
        "    print(f\"\\nTesting domain: {test_domain}\")\n",
        "    print(f\"\\nTop suspicious similar domains:\")\n",
        "    for doc in suspicious[:5]:\n",
        "        print(f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\")\n",
        "\n",
        "    print(f\"\\nTop trusted similar domains:\")\n",
        "    for doc in trusted[:5]:\n",
        "        print(f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Enhanced agent not yet created. Create it first with:\")\n",
        "    print(\"enhanced_agent = EnhancedEmailAgent(use_similarity_matching=True)\")"
      ],
      "metadata": {
        "id": "pSAAM6LT-bNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test various natural language queries\n",
        "parser = IntelligentQueryParser()\n",
        "intent = parser.parse_user_query(\"Show me urgent emails from last week\")\n",
        "print(intent.model_dump())  # Changed from .dict() to .model_dump()"
      ],
      "metadata": {
        "id": "0UCW1_ZU-bh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test complete pipeline\n",
        "queries = [\n",
        "    \"Find emails from this week about meetings\"\n",
        "]\n",
        "for query in queries:\n",
        "    result = enhanced_agent.process_emails(query)"
      ],
      "metadata": {
        "id": "hyz6UohP-boZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}