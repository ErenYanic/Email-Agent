{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC3DUSmHCOEk"
      },
      "outputs": [],
      "source": [
        "%pip install python-dotenv langchain langchain_community langchain-openai langsmith langgraph faiss-cpu beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Core imports and configuration**"
      ],
      "metadata": {
        "id": "RBwm4_L0aDgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Core imports and configuration - ASYNC VERSION\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import operator\n",
        "import asyncio  # Added for async support\n",
        "from typing import List, Dict, Any, Optional, Tuple, Annotated, TypedDict, Sequence\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain imports - same imports work for async\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.tools import tool, BaseTool\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage\n",
        "from langchain.schema import Document\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Email processing imports\n",
        "import base64\n",
        "from dateutil.parser import parse\n",
        "from langchain_community.agent_toolkits import GmailToolkit\n",
        "from langchain_community.tools.gmail.utils import build_resource_service\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure environment variables - exactly the same as sync version\n",
        "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
        "\n",
        "# Get the OpenAI API Key from the .env file\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "else:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in .env file!\")\n",
        "\n",
        "\n",
        "async def test_langsmith_connection():\n",
        "    \"\"\"\n",
        "    Asynchronously test the LangSmith connection and LLM.\n",
        "    This replaces the synchronous test in the original code.\n",
        "    \"\"\"\n",
        "    # Test the LangSmith connection\n",
        "    try:\n",
        "        from langsmith import Client\n",
        "        client = Client()\n",
        "        print(\"LangSmith connection successful!\")\n",
        "        print(f\"Project: {os.environ.get('LANGSMITH_PROJECT')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"LangSmith connection error: {e}\")\n",
        "\n",
        "    # Initialise test LLM\n",
        "    from langchain.chat_models import init_chat_model\n",
        "    test_llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "    # Send test message using async invocation\n",
        "    try:\n",
        "        # Using ainvoke for asynchronous invocation\n",
        "        response = await test_llm.ainvoke(\"Hi, this is a test message!\")\n",
        "        print(\"LLM test successful!\")\n",
        "        print(\"You can check the traces in LangSmith.\")\n",
        "        print(response)\n",
        "        del test_llm  # Clear memory after test\n",
        "    except Exception as e:\n",
        "        print(f\"LLM test error: {e}\")\n",
        "\n",
        "\n",
        "# Execute the async test function\n",
        "# Note: In Jupyter/Colab, we can use await directly, but for standalone scripts,\n",
        "# we would use asyncio.run()\n",
        "if __name__ == \"__main__\":\n",
        "    # For Jupyter/Colab environments\n",
        "    try:\n",
        "        # Check if we're in an async environment (Jupyter with IPython)\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()  # Allow nested async loops in Jupyter\n",
        "        asyncio.run(test_langsmith_connection())\n",
        "    except:\n",
        "        # For regular Python scripts or if nest_asyncio not available\n",
        "        loop = asyncio.get_event_loop()\n",
        "        loop.run_until_complete(test_langsmith_connection())\n",
        "\n",
        "print(\"‚úÖ Core configuration completed - ASYNC VERSION\")"
      ],
      "metadata": {
        "id": "FYGhn6SVM-4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Email Fetcher Class**"
      ],
      "metadata": {
        "id": "sTDzUW86aqyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Email Fetcher class - ASYNC VERSION with structured approach for email retrieval\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "class AsyncEmailFetcher:\n",
        "    \"\"\"\n",
        "    Asynchronous Gmail API email fetcher and processor class.\n",
        "    Uses async/await patterns for non-blocking email retrieval.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialise Gmail API service.\n",
        "        Note: The initialisation remains synchronous as it's a one-time setup.\n",
        "        \"\"\"\n",
        "        self.api_resource = build_resource_service()\n",
        "        self.toolkit = GmailToolkit(api_resource=self.api_resource)\n",
        "        self.search_tool = next(\n",
        "            (tool for tool in self.toolkit.get_tools() if tool.name == 'search_gmail'),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if not self.search_tool:\n",
        "            raise ValueError(\"Gmail search tool could not be initialised!\")\n",
        "\n",
        "        # Create a thread pool executor for running sync Gmail API calls\n",
        "        # This allows us to run blocking I/O operations asynchronously\n",
        "        self.executor = ThreadPoolExecutor(max_workers=10)\n",
        "\n",
        "    def get_email_contents(self, payload: Dict) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Recursively extract content from email payload.\n",
        "        Note: This remains synchronous as it's CPU-bound processing, not I/O.\n",
        "\n",
        "        Args:\n",
        "            payload: Email payload from Gmail API\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing 'text' and 'html' content\n",
        "        \"\"\"\n",
        "        plain_text = \"\"\n",
        "        html_text = \"\"\n",
        "\n",
        "        # Recursively process parts if they exist\n",
        "        if 'parts' in payload:\n",
        "            for part in payload['parts']:\n",
        "                nested_content = self.get_email_contents(part)\n",
        "                plain_text += nested_content['text']\n",
        "                html_text += nested_content['html']\n",
        "\n",
        "        # Decode body if present\n",
        "        elif 'body' in payload and 'data' in payload['body']:\n",
        "            mime_type = payload.get('mimeType', '')\n",
        "            body_data = payload['body'].get('data', '')\n",
        "\n",
        "            if body_data:\n",
        "                try:\n",
        "                    decoded_body = base64.urlsafe_b64decode(body_data).decode('utf-8', errors='ignore')\n",
        "                    if 'text/plain' in mime_type:\n",
        "                        plain_text += decoded_body\n",
        "                    elif 'text/html' in mime_type:\n",
        "                        html_text += decoded_body\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Decode error: {e}\")\n",
        "                    pass\n",
        "\n",
        "        return {'text': plain_text.strip(), 'html': html_text.strip()}\n",
        "\n",
        "    async def _search_emails_async(self, query: str, max_results: int) -> List:\n",
        "        \"\"\"\n",
        "        Asynchronously search for emails using Gmail API.\n",
        "        Wraps the synchronous search_tool.run() in an async executor.\n",
        "\n",
        "        Args:\n",
        "            query: Gmail search query\n",
        "            max_results: Maximum number of emails to fetch\n",
        "\n",
        "        Returns:\n",
        "            List of email search results\n",
        "        \"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        search_params = {\"query\": query, \"max_results\": max_results}\n",
        "\n",
        "        # Run the synchronous search_tool.run() in a thread pool\n",
        "        # This prevents blocking the event loop\n",
        "        search_results = await loop.run_in_executor(\n",
        "            self.executor,\n",
        "            self.search_tool.run,\n",
        "            search_params\n",
        "        )\n",
        "        return search_results\n",
        "\n",
        "    async def _fetch_email_detail_async(self, message_id: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Asynchronously fetch detailed information for a single email.\n",
        "\n",
        "        Args:\n",
        "            message_id: Gmail message ID\n",
        "\n",
        "        Returns:\n",
        "            Dict containing email details\n",
        "        \"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "\n",
        "        # Run the synchronous Gmail API call in a thread pool\n",
        "        message_detail = await loop.run_in_executor(\n",
        "            self.executor,\n",
        "            lambda: self.api_resource.users().messages().get(\n",
        "                userId='me',\n",
        "                id=message_id,\n",
        "                format='full'\n",
        "            ).execute()\n",
        "        )\n",
        "        return message_detail\n",
        "\n",
        "    async def _process_single_email(self, summary: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Process a single email asynchronously.\n",
        "        Extracts all metadata and content from an email.\n",
        "\n",
        "        Args:\n",
        "            summary: Email summary from search results\n",
        "\n",
        "        Returns:\n",
        "            Dict containing processed email data\n",
        "        \"\"\"\n",
        "        message_id = summary.get('id')\n",
        "\n",
        "        # Fetch email details asynchronously\n",
        "        message_detail = await self._fetch_email_detail_async(message_id)\n",
        "\n",
        "        payload = message_detail.get('payload', {})\n",
        "        headers = payload.get('headers', [])\n",
        "\n",
        "        # Extract header information (synchronous as it's quick processing)\n",
        "        subject = next((h['value'] for h in headers if h['name'].lower() == 'subject'), 'N/A')\n",
        "        sender = next((h['value'] for h in headers if h['name'].lower() == 'from'), 'N/A')\n",
        "        to = next((h['value'] for h in headers if h['name'].lower() == 'to'), 'N/A')\n",
        "        cc = next((h['value'] for h in headers if h['name'].lower() == 'cc'), 'N/A')\n",
        "        date_str = next((h['value'] for h in headers if h['name'].lower() == 'date'), None)\n",
        "\n",
        "        # Parse date\n",
        "        try:\n",
        "            email_date = parse(date_str) if date_str else None\n",
        "        except:\n",
        "            email_date = None\n",
        "\n",
        "        # Check labels and read status\n",
        "        labels = message_detail.get('labelIds', [])\n",
        "        is_unread = 'UNREAD' in labels\n",
        "\n",
        "        # Extract content - FULL TEXT, no truncation for security\n",
        "        contents = self.get_email_contents(payload)\n",
        "\n",
        "        # Recursive attachment check\n",
        "        has_attachment = False\n",
        "        attachment_names = []\n",
        "\n",
        "        def check_attachments(part):\n",
        "            \"\"\"Recursive attachment check\"\"\"\n",
        "            if part.get('filename'):\n",
        "                return True, part.get('filename')\n",
        "            if 'parts' in part:\n",
        "                for subpart in part['parts']:\n",
        "                    has_att, filename = check_attachments(subpart)\n",
        "                    if has_att:\n",
        "                        return True, filename\n",
        "            return False, None\n",
        "\n",
        "        if 'parts' in payload:\n",
        "            for part in payload['parts']:\n",
        "                has_att, filename = check_attachments(part)\n",
        "                if has_att:\n",
        "                    has_attachment = True\n",
        "                    if filename:\n",
        "                        attachment_names.append(filename)\n",
        "\n",
        "        # Return processed email data - storing FULL content\n",
        "        return {\n",
        "            'id': message_id,\n",
        "            'is_unread': is_unread,\n",
        "            'date': email_date,\n",
        "            'from': sender,\n",
        "            'to': to,\n",
        "            'cc': cc,\n",
        "            'labels': labels,\n",
        "            'subject': subject,\n",
        "            'body_text': contents['text'],  # Full text for analysis\n",
        "            'body_html': contents['html'],  # Full HTML for security checks\n",
        "            'has_attachment': has_attachment,\n",
        "            'attachment_names': attachment_names\n",
        "        }\n",
        "\n",
        "    async def fetch_emails(self, query: str = \"in:inbox\", max_results: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Asynchronously fetch emails from Gmail and return as DataFrame.\n",
        "        Uses concurrent processing for improved performance on multiple emails.\n",
        "\n",
        "        Args:\n",
        "            query: Gmail search query\n",
        "            max_results: Maximum number of emails to fetch\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Email data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Search for emails asynchronously\n",
        "            print(f\"üîß Searching for emails with query: '{query}'...\")\n",
        "            search_results = await self._search_emails_async(query, max_results)\n",
        "\n",
        "            if not search_results:\n",
        "                print(\"‚ö†Ô∏è No emails found\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            print(f\"üìß Found {len(search_results)} emails, fetching details asynchronously...\")\n",
        "\n",
        "            # Process all emails concurrently using asyncio.gather\n",
        "            # This is the main performance improvement - all emails are fetched in parallel\n",
        "            tasks = [self._process_single_email(summary) for summary in search_results]\n",
        "            processed_emails = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "            # Filter out any exceptions and keep only successful results\n",
        "            valid_emails = []\n",
        "            for email in processed_emails:\n",
        "                if isinstance(email, dict):  # Successfully processed\n",
        "                    valid_emails.append(email)\n",
        "                elif isinstance(email, Exception):  # Handle any errors\n",
        "                    print(f\"‚ö†Ô∏è Error processing email: {email}\")\n",
        "\n",
        "            # Create DataFrame\n",
        "            if valid_emails:\n",
        "                df = pd.DataFrame(valid_emails)\n",
        "                df = df.sort_values(by='date', ascending=False, na_position='last').reset_index(drop=True)\n",
        "                print(f\"‚úÖ Successfully processed {len(df)} emails asynchronously\")\n",
        "                return df\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No emails could be processed\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Email fetch error: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    async def save_to_csv(self, df: pd.DataFrame, filename: str = 'fetched_emails.csv'):\n",
        "        \"\"\"\n",
        "        Asynchronously save DataFrame to CSV file.\n",
        "        Uses async file I/O for non-blocking operation.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to save\n",
        "            filename: Output filename\n",
        "        \"\"\"\n",
        "        if not df.empty:\n",
        "            loop = asyncio.get_event_loop()\n",
        "\n",
        "            # Run the blocking I/O operation in a thread pool\n",
        "            await loop.run_in_executor(\n",
        "                self.executor,\n",
        "                lambda: df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "            )\n",
        "            print(f\"‚úÖ DataFrame successfully saved to '{filename}'\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No data to save\")\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup method to properly shutdown the thread pool executor.\"\"\"\n",
        "        if hasattr(self, 'executor'):\n",
        "            self.executor.shutdown(wait=False)\n",
        "\n",
        "\n",
        "# Async test function for the fetcher\n",
        "async def test_async_email_fetcher():\n",
        "    \"\"\"Test the AsyncEmailFetcher with a sample query.\"\"\"\n",
        "    email_fetcher = AsyncEmailFetcher()\n",
        "    print(\"‚úÖ AsyncEmailFetcher successfully created\")\n",
        "\n",
        "    # Example of how to use the async fetcher\n",
        "    # df = await email_fetcher.fetch_emails(\"in:inbox\", max_results=5)\n",
        "    # if not df.empty:\n",
        "    #     await email_fetcher.save_to_csv(df, 'async_test_emails.csv')\n",
        "\n",
        "    return email_fetcher\n",
        "\n",
        "\n",
        "# Execute the test\n",
        "if __name__ == \"__main__\":\n",
        "    # For Jupyter/Colab environments\n",
        "    try:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        email_fetcher = asyncio.run(test_async_email_fetcher())\n",
        "    except:\n",
        "        # For regular Python scripts\n",
        "        loop = asyncio.get_event_loop()\n",
        "        email_fetcher = loop.run_until_complete(test_async_email_fetcher())\n",
        "\n",
        "print(\"‚úÖ AsyncEmailFetcher ready for use\")"
      ],
      "metadata": {
        "id": "pRE-K7LXPHV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2a: Enhanced Query Parser for Dynamic Email Retrieval - ASYNC VERSION\n",
        "\n",
        "class EmailQueryIntent(BaseModel):\n",
        "    \"\"\"\n",
        "    Structured output for email query parsing.\n",
        "    Note: This model remains the same for both sync and async versions.\n",
        "    \"\"\"\n",
        "\n",
        "    email_count: Optional[int] = Field(\n",
        "        description=\"Number of emails to fetch (None means all matching emails)\"\n",
        "    )\n",
        "    time_filter: Optional[str] = Field(\n",
        "        description=\"Time-based filter (e.g., 'today', 'this_week', 'last_month')\"\n",
        "    )\n",
        "    status_filter: Optional[str] = Field(\n",
        "        description=\"Email status filter (e.g., 'unread', 'important', 'starred')\"\n",
        "    )\n",
        "    sender_filter: Optional[str] = Field(\n",
        "        description=\"Specific sender to filter by\"\n",
        "    )\n",
        "    subject_keywords: Optional[str] = Field(\n",
        "        description=\"Keywords to search in subject\"\n",
        "    )\n",
        "    label_filter: Optional[str] = Field(\n",
        "        description=\"Gmail label to filter by\"\n",
        "    )\n",
        "\n",
        "\n",
        "class AsyncIntelligentQueryParser:\n",
        "    \"\"\"\n",
        "    Asynchronously parse user queries to extract email retrieval parameters dynamically.\n",
        "    Uses async LLM calls for natural language understanding rather than rigid rules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_model: str = \"gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Initialise the query parser with LLM.\n",
        "\n",
        "        Args:\n",
        "            llm_model: Model to use for parsing\n",
        "        \"\"\"\n",
        "        self.llm = ChatOpenAI(model=llm_model, temperature=0.3)\n",
        "        self.output_parser = PydanticOutputParser(pydantic_object=EmailQueryIntent)\n",
        "\n",
        "    async def parse_user_query(self, user_query: str) -> EmailQueryIntent:\n",
        "        \"\"\"\n",
        "        Asynchronously parse user query to extract email retrieval parameters.\n",
        "        Uses async LLM invocation for non-blocking operation.\n",
        "\n",
        "        Args:\n",
        "            user_query: Natural language query from user\n",
        "\n",
        "        Returns:\n",
        "            EmailQueryIntent with parsed parameters\n",
        "        \"\"\"\n",
        "\n",
        "        format_instructions = self.output_parser.get_format_instructions()\n",
        "\n",
        "        prompt = f\"\"\"Parse the following email query to extract retrieval parameters.\n",
        "\n",
        "User Query: \"{user_query}\"\n",
        "\n",
        "Extract the following information:\n",
        "- email_count: If user specifies a number (e.g., \"5 emails\", \"last 10\"), extract it.\n",
        "  If they say \"all\", set to None. If no number mentioned, set to None.\n",
        "- time_filter: Extract time references like \"today\", \"yesterday\", \"this week\", \"last week\", \"this month\", \"last month\"\n",
        "  IMPORTANT: Preserve the exact format (e.g., \"this week\" not \"this_week\")\n",
        "- status_filter: Extract status like \"unread\", \"important\", \"starred\"\n",
        "  NOTE: If user says \"urgent\" or \"priority\", map it to \"important\"\n",
        "- sender_filter: Extract specific sender if mentioned (name or email)\n",
        "- subject_keywords: Extract any subject-related keywords or topics\n",
        "- label_filter: Extract Gmail labels if mentioned\n",
        "\n",
        "Examples:\n",
        "- \"Show me my last 5 unread emails\" ‚Üí email_count: 5, status_filter: \"unread\"\n",
        "- \"Get all emails from today\" ‚Üí email_count: None, time_filter: \"today\"\n",
        "- \"Fetch emails about the project\" ‚Üí subject_keywords: \"project\"\n",
        "- \"Show all unread emails\" ‚Üí email_count: None, status_filter: \"unread\"\n",
        "- \"Get all urgent emails from today\" ‚Üí email_count: None, time_filter: \"today\", status_filter: \"important\"\n",
        "- \"Find emails from this week about meetings\" ‚Üí time_filter: \"this week\", subject_keywords: \"meetings\"\n",
        "- \"Urgent messages from John\" ‚Üí status_filter: \"important\", sender_filter: \"John\"\n",
        "- \"Show me priority emails\" ‚Üí status_filter: \"important\"\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Parse the query and respond with the JSON object:\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Use async invocation for the LLM\n",
        "            response = await self.llm.ainvoke(prompt)\n",
        "\n",
        "            # Parse the response\n",
        "            intent = self.output_parser.parse(response.content)\n",
        "            return intent\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to parse query intent: {e}\")\n",
        "            # Return default intent\n",
        "            return EmailQueryIntent()\n",
        "\n",
        "    def build_gmail_query(self, intent: EmailQueryIntent) -> tuple[str, int]:\n",
        "        \"\"\"\n",
        "        Build Gmail API query string from parsed intent.\n",
        "        Note: This remains synchronous as it's pure computation, no I/O involved.\n",
        "\n",
        "        Args:\n",
        "            intent: Parsed email query intent\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (gmail_query_string, max_results)\n",
        "        \"\"\"\n",
        "        query_parts = []\n",
        "\n",
        "        # Build query based on intent\n",
        "        if intent.status_filter:\n",
        "            # IMPROVEMENT 1: Map common terms to Gmail's actual filters\n",
        "            status_mappings = {\n",
        "                \"unread\": \"is:unread\",\n",
        "                \"important\": \"is:important\",\n",
        "                \"urgent\": \"is:important\",  # Map \"urgent\" to \"important\"\n",
        "                \"priority\": \"is:important\",  # Map \"priority\" to \"important\"\n",
        "                \"starred\": \"is:starred\",\n",
        "                \"read\": \"-is:unread\",  # Negative filter for read emails\n",
        "            }\n",
        "\n",
        "            filter_value = intent.status_filter.lower()\n",
        "            if filter_value in status_mappings:\n",
        "                query_parts.append(status_mappings[filter_value])\n",
        "            else:\n",
        "                # Try to use it as-is if not in mappings\n",
        "                query_parts.append(f\"is:{intent.status_filter}\")\n",
        "\n",
        "        if intent.time_filter:\n",
        "            # IMPROVEMENT 2: Handle both \"this week\" and \"this_week\" formats\n",
        "            time_mappings = {\n",
        "                \"today\": \"newer_than:1d\",\n",
        "                \"yesterday\": \"older_than:1d newer_than:2d\",\n",
        "                \"this week\": \"newer_than:7d\",  # Handle space version\n",
        "                \"this_week\": \"newer_than:7d\",  # Handle underscore version\n",
        "                \"last week\": \"older_than:7d newer_than:14d\",\n",
        "                \"last_week\": \"older_than:7d newer_than:14d\",\n",
        "                \"this month\": \"newer_than:30d\",\n",
        "                \"this_month\": \"newer_than:30d\",\n",
        "                \"last month\": \"older_than:30d newer_than:60d\",\n",
        "                \"last_month\": \"older_than:30d newer_than:60d\"\n",
        "            }\n",
        "\n",
        "            filter_value = intent.time_filter.lower()\n",
        "            if filter_value in time_mappings:\n",
        "                query_parts.append(time_mappings[filter_value])\n",
        "\n",
        "        if intent.sender_filter:\n",
        "            # IMPROVEMENT 3: Handle sender filter more intelligently\n",
        "            sender = intent.sender_filter.strip()\n",
        "            if '@' in sender:\n",
        "                query_parts.append(f\"from:{sender}\")\n",
        "            else:\n",
        "                # If no @, treat as name/partial match\n",
        "                query_parts.append(f\"from:{sender}\")\n",
        "\n",
        "        if intent.subject_keywords:\n",
        "            # IMPROVEMENT 4: Better handling of multi-word subjects\n",
        "            keywords = intent.subject_keywords.strip()\n",
        "            if ' ' in keywords:\n",
        "                # Multiple words - search for the phrase\n",
        "                query_parts.append(f'subject:\"{keywords}\"')\n",
        "            else:\n",
        "                # Single word\n",
        "                query_parts.append(f\"subject:{keywords}\")\n",
        "\n",
        "        if intent.label_filter:\n",
        "            label = intent.label_filter.strip()\n",
        "            query_parts.append(f\"label:{label}\")\n",
        "\n",
        "        # Default to inbox if no specific filters\n",
        "        if not query_parts:\n",
        "            query_parts.append(\"in:inbox\")\n",
        "\n",
        "        # Build the final query\n",
        "        gmail_query = \" \".join(query_parts)\n",
        "\n",
        "        # Determine max_results\n",
        "        # If email_count is None, fetch all (set a reasonable limit like 100)\n",
        "        # If specified, use that number\n",
        "        max_results = intent.email_count if intent.email_count else 100\n",
        "\n",
        "        return gmail_query, max_results\n",
        "\n",
        "\n",
        "# Enhanced async fetch method for EmailFetcher\n",
        "async def enhanced_fetch_emails_node_async(email_agent_instance, state):\n",
        "    \"\"\"\n",
        "    Enhanced asynchronous email fetching with dynamic query parsing.\n",
        "    This async version replaces the rigid if-elif logic with intelligent parsing\n",
        "    and uses concurrent operations for better performance.\n",
        "\n",
        "    Args:\n",
        "        email_agent_instance: Instance of the email agent with async fetcher\n",
        "        state: Current state dictionary\n",
        "\n",
        "    Returns:\n",
        "        Updated state with fetched emails\n",
        "    \"\"\"\n",
        "    print(\"üîß Intelligently parsing email request (async)...\")\n",
        "\n",
        "    # Get the original user message\n",
        "    user_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
        "\n",
        "    # Initialise async parser\n",
        "    parser = AsyncIntelligentQueryParser()\n",
        "\n",
        "    # Parse the query asynchronously\n",
        "    intent = await parser.parse_user_query(user_message)\n",
        "    print(f\"  Parsed intent: {intent.model_dump()}\")\n",
        "\n",
        "    # Build Gmail query (synchronous as it's just string manipulation)\n",
        "    gmail_query, max_results = parser.build_gmail_query(intent)\n",
        "    print(f\"  Gmail query: '{gmail_query}' (max: {max_results})\")\n",
        "\n",
        "    try:\n",
        "        # Fetch emails using the async fetcher with parsed parameters\n",
        "        # This is the key async improvement - non-blocking email fetch\n",
        "        df = await email_agent_instance.email_fetcher.fetch_emails(\n",
        "            query=gmail_query,\n",
        "            max_results=max_results\n",
        "        )\n",
        "\n",
        "        if not df.empty:\n",
        "            emails = df.to_dict('records')\n",
        "            state[\"email_data\"] = {\n",
        "                \"emails\": emails,\n",
        "                \"count\": len(emails),\n",
        "                \"query\": gmail_query,\n",
        "                \"intent\": intent.model_dump()\n",
        "            }\n",
        "            print(f\"  ‚úÖ Found {len(emails)} emails\")\n",
        "        else:\n",
        "            state[\"email_data\"] = {\n",
        "                \"emails\": [],\n",
        "                \"count\": 0,\n",
        "                \"query\": gmail_query,\n",
        "                \"intent\": intent.model_dump()\n",
        "            }\n",
        "            print(\"  ‚ö†Ô∏è No emails found matching criteria\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"email_data\"] = {\"error\": str(e)}\n",
        "        print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# Test function for the async query parser\n",
        "async def test_async_query_parser():\n",
        "    \"\"\"Test the AsyncIntelligentQueryParser with sample queries.\"\"\"\n",
        "    parser = AsyncIntelligentQueryParser()\n",
        "\n",
        "    # Test with a sample query\n",
        "    test_query = \"Show me my last 5 unread emails\"\n",
        "    intent = await parser.parse_user_query(test_query)\n",
        "    print(f\"Test query: '{test_query}'\")\n",
        "    print(f\"Parsed intent: {intent.model_dump()}\")\n",
        "\n",
        "    # Build Gmail query\n",
        "    gmail_query, max_results = parser.build_gmail_query(intent)\n",
        "    print(f\"Gmail query: '{gmail_query}' (max: {max_results})\")\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "# Execute the test\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        parser = asyncio.run(test_async_query_parser())\n",
        "    except:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        parser = loop.run_until_complete(test_async_query_parser())\n",
        "\n",
        "print(\"‚úÖ Async Intelligent query parser created\")"
      ],
      "metadata": {
        "id": "_GveR1ErUUDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Security Analysis Module**"
      ],
      "metadata": {
        "id": "ogkvJZGBiUPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Security analysis module with hybrid LLM integration - ASYNC VERSION\n",
        "\n",
        "class AsyncSecurityAnalyser:\n",
        "    \"\"\"\n",
        "    Asynchronous email security analysis class with hybrid deterministic + LLM approach.\n",
        "    Uses async LLM calls for improved performance when analysing multiple emails.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_model=None):\n",
        "        \"\"\"\n",
        "        Initialise threat patterns and LLM integration.\n",
        "        Note: Initialisation remains synchronous as it's one-time setup.\n",
        "        \"\"\"\n",
        "\n",
        "        # LLM for domain assessment\n",
        "        self.security_llm = ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # Using lighter model for quick assessments\n",
        "            temperature=0.1,  # Low temperature for consistent security decisions\n",
        "        )\n",
        "\n",
        "        # Phishing/Scam patterns - same as sync version\n",
        "        self.phishing_patterns = [\n",
        "            # URL patterns\n",
        "            r'bit\\.ly|tinyurl|short\\.link|clck\\.ru',  # URL shorteners\n",
        "            r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}',  # IP addresses\n",
        "            r'@[^@]*@',  # Double @ signs\n",
        "\n",
        "            # Word patterns\n",
        "            r'urgent.{0,20}action.{0,20}required',\n",
        "            r'verify.{0,20}account.{0,20}immediately',\n",
        "            r'suspended.{0,20}account',\n",
        "            r'click.{0,20}here.{0,20}immediately',\n",
        "            r'limited.{0,20}time.{0,20}offer',\n",
        "            r'congratulations.{0,20}won',\n",
        "            r'claim.{0,20}prize',\n",
        "            r'tax.{0,20}refund',\n",
        "            r'nigerian?.{0,20}prince',\n",
        "            r'claim.{0,20}reward',\n",
        "        ]\n",
        "\n",
        "        # Prompt injection patterns - same as sync version\n",
        "        self.injection_patterns = [\n",
        "            r'ignore.{0,20}previous.{0,20}instructions',\n",
        "            r'disregard.{0,20}all.{0,20}prior',\n",
        "            r'forget.{0,20}everything',\n",
        "            r'new.{0,20}instructions.{0,20}follow',\n",
        "            r'system.{0,20}prompt.{0,20}override',\n",
        "            r'admin.{0,20}mode',\n",
        "            r'developer.{0,20}mode',\n",
        "            r'bypass.{0,20}security',\n",
        "            r'<script',  # XSS attempts\n",
        "            r'javascript:',\n",
        "            r'eval\\(',\n",
        "            r'onerror=',\n",
        "        ]\n",
        "\n",
        "    async def assess_domain_with_llm(self, domain: str, email_context: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Asynchronously use LLM to assess domain trustworthiness.\n",
        "        This method will be enhanced by DomainSimilarityMatcher if enabled.\n",
        "\n",
        "        Args:\n",
        "            domain: Domain to assess\n",
        "            email_context: Email context for better assessment\n",
        "\n",
        "        Returns:\n",
        "            Dict with assessment results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"Analyse this email sender domain for security risks.\n",
        "\n",
        "Domain: {domain}\n",
        "Email Subject: {email_context.get('subject', 'N/A')}\n",
        "Sender Full Address: {email_context.get('from', 'N/A')}\n",
        "\n",
        "Based on the domain name pattern and common phishing tactics, assess if this domain appears:\n",
        "1. SUSPICIOUS (likely phishing/scam)\n",
        "2. TRUSTED (legitimate business/service)\n",
        "3. UNKNOWN (cannot determine)\n",
        "\n",
        "Consider:\n",
        "- Does the domain mimic known brands?\n",
        "- Does it use suspicious patterns?\n",
        "- Is it a legitimate business domain?\n",
        "\n",
        "Respond with ONLY one word: SUSPICIOUS, TRUSTED, or UNKNOWN\n",
        "\n",
        "CRITICAL NOTE: Try not to mark as 'UNKNOWN' as much as possible.\n",
        "\n",
        "Decision:\"\"\"\n",
        "\n",
        "            # Use async invocation for non-blocking LLM call\n",
        "            response = await self.security_llm.ainvoke(prompt)\n",
        "            assessment = response.content.strip().upper()\n",
        "\n",
        "            # Ensure assessment is one of the expected values\n",
        "            if assessment not in ['SUSPICIOUS', 'TRUSTED', 'UNKNOWN']:\n",
        "                assessment = 'UNKNOWN'\n",
        "\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': assessment,\n",
        "                'confidence': 'low',  # Low confidence without similarity matching\n",
        "                'method': 'basic_llm'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è LLM assessment failed for {domain}: {e}\")\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': 'UNKNOWN',\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "    async def check_phishing_indicators(self, email_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Check for phishing indicators with hybrid approach.\n",
        "        Now async to support async LLM domain assessment.\n",
        "\n",
        "        Args:\n",
        "            email_data: Email data dictionary\n",
        "\n",
        "        Returns:\n",
        "            Dict with phishing analysis results\n",
        "        \"\"\"\n",
        "        indicators = []\n",
        "        risk_score = 0\n",
        "        domain_assessment = {}\n",
        "\n",
        "        # Analyse full text (synchronous pattern matching)\n",
        "        text = f\"{email_data.get('subject', '')} {email_data.get('body_text', '')} {email_data.get('body_html', '')}\".lower()\n",
        "\n",
        "        # Pattern checking (remains synchronous as it's CPU-bound)\n",
        "        for pattern in self.phishing_patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                indicators.append(f\"Suspicious pattern detected: {pattern}\")\n",
        "                risk_score += 20\n",
        "\n",
        "        # Urgency words analysis (synchronous)\n",
        "        urgency_words = ['urgent', 'immediate', 'expire', 'suspend', 'limited time']\n",
        "        urgency_count = sum(1 for word in urgency_words if word in text)\n",
        "        if urgency_count > 2:\n",
        "            indicators.append(f\"High urgency level detected ({urgency_count} keywords)\")\n",
        "            risk_score += urgency_count * 10\n",
        "\n",
        "        # Enhanced sender analysis\n",
        "        sender = email_data.get('from', '')\n",
        "        sender_domain = sender.split('@')[-1].split('>')[0] if '@' in sender else ''\n",
        "\n",
        "        if sender_domain:\n",
        "            # Check display name vs actual email (synchronous)\n",
        "            if '<' in sender and '>' in sender:\n",
        "                display_name = sender.split('<')[0].strip()\n",
        "                actual_email = sender.split('<')[1].split('>')[0]\n",
        "\n",
        "                # Check if display name contains different email\n",
        "                if '@' in display_name:\n",
        "                    indicators.append(\"Display name contains different email address\")\n",
        "                    risk_score += 30\n",
        "\n",
        "            # Domain assessment - now uses async LLM method\n",
        "            llm_result = await self.assess_domain_with_llm(sender_domain, email_data)\n",
        "            domain_assessment = {\n",
        "                'status': llm_result['llm_assessment'].lower(),\n",
        "                'source': llm_result.get('method', 'llm'),\n",
        "                'confidence': llm_result.get('confidence', 'low')\n",
        "            }\n",
        "\n",
        "            if llm_result['llm_assessment'] == 'SUSPICIOUS':\n",
        "                indicators.append(f\"Domain assessed as suspicious: {sender_domain}\")\n",
        "                risk_score += 30\n",
        "\n",
        "            elif llm_result['llm_assessment'] == 'TRUSTED':\n",
        "                indicators.append(f\"Domain assessed as trusted: {sender_domain}\")\n",
        "                risk_score -= 10\n",
        "                risk_score = max(0, risk_score)\n",
        "\n",
        "            else:  # UNKNOWN\n",
        "                indicators.append(f\"Domain assessment inconclusive: {sender_domain}\")\n",
        "                risk_score += 10\n",
        "\n",
        "        # Attachment analysis (synchronous)\n",
        "        if email_data.get('has_attachment'):\n",
        "            attachments = email_data.get('attachment_names', [])\n",
        "            dangerous_extensions = ['.exe', '.zip', '.rar', '.bat', '.cmd', '.scr', '.vbs']\n",
        "\n",
        "            for att in attachments:\n",
        "                if any(att.lower().endswith(ext) for ext in dangerous_extensions):\n",
        "                    indicators.append(f\"Dangerous file extension detected: {att}\")\n",
        "                    risk_score += 40\n",
        "\n",
        "        return {\n",
        "            'indicators': indicators,\n",
        "            'risk_score': min(risk_score, 100),\n",
        "            'risk_level': self._calculate_risk_level(risk_score),\n",
        "            'domain_assessment': domain_assessment\n",
        "        }\n",
        "\n",
        "    def check_url_safety(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Check URL safety in email content.\n",
        "        Remains synchronous as it's pattern matching without I/O.\n",
        "\n",
        "        Args:\n",
        "            text: Text to check for URLs\n",
        "\n",
        "        Returns:\n",
        "            Dict with URL analysis results\n",
        "        \"\"\"\n",
        "        urls = re.findall(r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+', text)\n",
        "        suspicious_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            # Check for URL shorteners\n",
        "            if any(short in url.lower() for short in ['bit.ly', 'tinyurl', 'short.link']):\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'URL shortener detected - could hide malicious destination'\n",
        "                })\n",
        "\n",
        "            # Check for IP addresses instead of domains\n",
        "            if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url):\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'Contains IP address instead of domain name'\n",
        "                })\n",
        "\n",
        "            # Check for homograph attacks (similar looking characters)\n",
        "            if any(char in url for char in ['–∞', '–µ', '–æ', '—Ä', '—Å', '—É', '—Ö']):  # Cyrillic chars\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'Possible homograph attack - contains lookalike characters'\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'total_urls': len(urls),\n",
        "            'suspicious_urls': suspicious_urls,\n",
        "            'risk_level': 'high' if suspicious_urls else 'low'\n",
        "        }\n",
        "\n",
        "    def check_prompt_injection(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Check for prompt injection attempts.\n",
        "        Remains synchronous as it's pattern matching without I/O.\n",
        "\n",
        "        Args:\n",
        "            text: Text to check for injection patterns\n",
        "\n",
        "        Returns:\n",
        "            Dict with injection analysis results\n",
        "        \"\"\"\n",
        "        injections_found = []\n",
        "\n",
        "        for pattern in self.injection_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                injections_found.append({\n",
        "                    'pattern': pattern,\n",
        "                    'matches': matches[:3]  # First 3 matches\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'injection_detected': len(injections_found) > 0,\n",
        "            'injection_patterns': injections_found,\n",
        "            'risk_level': 'critical' if len(injections_found) > 2 else\n",
        "                         'high' if len(injections_found) > 0 else 'none'\n",
        "        }\n",
        "\n",
        "    def _calculate_risk_level(self, score: int) -> str:\n",
        "        \"\"\"\n",
        "        Calculate risk level from score.\n",
        "        Remains synchronous as it's pure computation.\n",
        "\n",
        "        Args:\n",
        "            score: Risk score\n",
        "\n",
        "        Returns:\n",
        "            Risk level string\n",
        "        \"\"\"\n",
        "        if score >= 70:\n",
        "            return 'critical'\n",
        "        elif score >= 50:\n",
        "            return 'high'\n",
        "        elif score >= 30:\n",
        "            return 'medium'\n",
        "        elif score >= 10:\n",
        "            return 'low'\n",
        "        else:\n",
        "            return 'safe'\n",
        "\n",
        "    async def analyse_email_security(self, email_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Complete async security analysis with hybrid approach.\n",
        "        Main method is now async to support async phishing check.\n",
        "\n",
        "        Args:\n",
        "            email_data: Email data to analyse\n",
        "\n",
        "        Returns:\n",
        "            Dict with complete security analysis\n",
        "        \"\"\"\n",
        "\n",
        "        # Run all analyses using FULL text\n",
        "        full_text = f\"{email_data.get('subject', '')} {email_data.get('body_text', '')} {email_data.get('body_html', '')}\"\n",
        "\n",
        "        # Run analyses - phishing check is now async\n",
        "        url_analysis = self.check_url_safety(full_text)  # Sync\n",
        "        phishing_analysis = await self.check_phishing_indicators(email_data)  # Async\n",
        "        injection_analysis = self.check_prompt_injection(full_text)  # Sync\n",
        "\n",
        "        # Calculate overall risk score\n",
        "        overall_risk_score = phishing_analysis['risk_score']\n",
        "\n",
        "        if url_analysis['risk_level'] == 'high':\n",
        "            overall_risk_score = min(overall_risk_score + 30, 100)\n",
        "\n",
        "        if injection_analysis['risk_level'] == 'critical':\n",
        "            overall_risk_score = min(overall_risk_score + 50, 100)\n",
        "        elif injection_analysis['risk_level'] == 'high':\n",
        "            overall_risk_score = min(overall_risk_score + 30, 100)\n",
        "\n",
        "        return {\n",
        "            'email_id': email_data.get('id'),\n",
        "            'subject': email_data.get('subject'),\n",
        "            'sender': email_data.get('from'),\n",
        "            'overall_risk_score': overall_risk_score,\n",
        "            'overall_risk_level': self._calculate_risk_level(overall_risk_score),\n",
        "            'domain_assessment': phishing_analysis.get('domain_assessment', {}),\n",
        "            'url_analysis': url_analysis,\n",
        "            'phishing_analysis': phishing_analysis,\n",
        "            'injection_analysis': injection_analysis,\n",
        "            'recommendations': self._generate_recommendations(\n",
        "                overall_risk_score,\n",
        "                phishing_analysis,\n",
        "                injection_analysis\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def _generate_recommendations(self, risk_score: int, phishing: Dict, injection: Dict) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate security recommendations.\n",
        "        Remains synchronous as it's pure logic without I/O.\n",
        "\n",
        "        Args:\n",
        "            risk_score: Overall risk score\n",
        "            phishing: Phishing analysis results\n",
        "            injection: Injection analysis results\n",
        "\n",
        "        Returns:\n",
        "            List of recommendations\n",
        "        \"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        if risk_score >= 70:\n",
        "            recommendations.append(\"‚õî CRITICAL: Do NOT open this email - DELETE immediately!\")\n",
        "            recommendations.append(\"üö® Report to IT security team\")\n",
        "        elif risk_score >= 50:\n",
        "            recommendations.append(\"‚ö†Ô∏è HIGH RISK: Do not click any links\")\n",
        "            recommendations.append(\"üîß Verify sender identity independently\")\n",
        "        elif risk_score >= 30:\n",
        "            recommendations.append(\"‚ö° CAUTION: Suspicious content detected\")\n",
        "\n",
        "        if injection['injection_detected']:\n",
        "            recommendations.append(\"ü§ñ PROMPT INJECTION detected - do not copy to AI systems\")\n",
        "\n",
        "        if phishing['indicators']:\n",
        "            recommendations.append(\"üé£ Phishing indicators detected - do not share personal information\")\n",
        "\n",
        "        # Add domain-specific recommendation\n",
        "        domain_assessment = phishing.get('domain_assessment', {})\n",
        "        if domain_assessment.get('confidence') == 'high' and domain_assessment.get('status') == 'suspicious':\n",
        "            recommendations.append(\"üîç AI assessment with similarity matching suggests sender domain is suspicious\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "\n",
        "# Test function for the async security analyser\n",
        "async def test_async_security_analyser():\n",
        "    \"\"\"Test the AsyncSecurityAnalyser with a sample email.\"\"\"\n",
        "    security_analyser = AsyncSecurityAnalyser()\n",
        "    print(\"‚úÖ AsyncSecurityAnalyser created\")\n",
        "\n",
        "    # Test with a sample email data\n",
        "    test_email = {\n",
        "        'id': 'test123',\n",
        "        'subject': 'Urgent: Verify your account immediately',\n",
        "        'from': 'security@suspici0us-domain.com',\n",
        "        'body_text': 'Click here immediately to verify your account or it will be suspended.',\n",
        "        'body_html': '',\n",
        "        'has_attachment': False,\n",
        "        'attachment_names': []\n",
        "    }\n",
        "\n",
        "    # Run async security analysis\n",
        "    result = await security_analyser.analyse_email_security(test_email)\n",
        "    print(f\"Security analysis result: Risk Level = {result['overall_risk_level']}\")\n",
        "\n",
        "    return security_analyser\n",
        "\n",
        "\n",
        "# Execute the test\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        security_analyser = asyncio.run(test_async_security_analyser())\n",
        "    except:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        security_analyser = loop.run_until_complete(test_async_security_analyser())\n",
        "\n",
        "print(\"‚úÖ Cleaned AsyncSecurityAnalyser created - domain lists now managed in DomainSimilarityMatcher\")"
      ],
      "metadata": {
        "id": "m93GsI7TYVwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3a: Enhanced Domain Assessment with Similarity Matching - ASYNC VERSION\n",
        "\n",
        "class AsyncDomainSimilarityMatcher:\n",
        "    \"\"\"\n",
        "    Asynchronous domain similarity matcher using vector embeddings for intelligent domain assessment.\n",
        "    Uses semantic similarity to find the most relevant examples for LLM decision-making.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings_model: str = \"text-embedding-3-small\"):\n",
        "        \"\"\"\n",
        "        Initialise the domain similarity matcher with vector stores.\n",
        "        Note: Initialisation remains synchronous as it's one-time setup.\n",
        "\n",
        "        Args:\n",
        "            embeddings_model: OpenAI embeddings model to use\n",
        "        \"\"\"\n",
        "        self.embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
        "\n",
        "        # Suspicious domains with context - same as sync version\n",
        "        self.suspicious_domains_data = [\n",
        "            (\"109.197.125.34.bc.googleusercontent.com\", \"IP address subdomain impersonating Google\"),\n",
        "            (\"accounts-mail.ru\", \"Russian domain mimicking account services\"),\n",
        "            (\"adobe-jkwefnewkjnfkjewnfkejwnfkjew.pages.dev\", \"Gibberish subdomain impersonating Adobe\"),\n",
        "            (\"business-facebook-covid19.com\", \"COVID-19 phishing using Facebook brand\"),\n",
        "            (\"google-secure.org\", \"Fake Google security domain\"),\n",
        "            (\"disceord.gift\", \"Discord typosquatting with gift scam\"),\n",
        "            (\"microsoft-error-pages-check-errors.pages.dev\", \"Fake Microsoft error page\"),\n",
        "            (\"steamcommunitc.com\", \"Steam typosquatting domain\"),\n",
        "            (\"xn--gmai-88b.com\", \"Homograph attack on Gmail\"),\n",
        "            (\"trust.twallet.cam\", \"Trust Wallet phishing domain\"),\n",
        "            # Add more as needed\n",
        "        ]\n",
        "\n",
        "        # Trusted domains with context - same as sync version\n",
        "        self.trusted_domains_data = [\n",
        "            (\"gmail.com\", \"Official Google email service\"),\n",
        "            (\"outlook.com\", \"Microsoft email service\"),\n",
        "            (\"amazon.com\", \"Official Amazon domain\"),\n",
        "            (\"github.com\", \"Official GitHub platform\"),\n",
        "            (\"linkedin.com\", \"Professional networking platform\"),\n",
        "            (\"paypal.com\", \"Official PayPal payment service\"),\n",
        "            (\"dropbox.com\", \"Cloud storage service\"),\n",
        "            (\"slack.com\", \"Team collaboration platform\"),\n",
        "            (\"zoom.us\", \"Video conferencing service\"),\n",
        "            (\"adobe.com\", \"Official Adobe domain\"),\n",
        "            # Add more as needed\n",
        "        ]\n",
        "\n",
        "        # Create vector stores (synchronous initialisation)\n",
        "        self._initialise_vector_stores()\n",
        "\n",
        "        # Create executor for async vector operations if needed\n",
        "        self.executor = ThreadPoolExecutor(max_workers=5)\n",
        "\n",
        "    def _initialise_vector_stores(self):\n",
        "        \"\"\"\n",
        "        Create FAISS vector stores for domain similarity search.\n",
        "        Remains synchronous as it's one-time setup during initialisation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create documents for suspicious domains\n",
        "        suspicious_docs = [\n",
        "            Document(\n",
        "                page_content=f\"{domain} - {context}\",\n",
        "                metadata={\"domain\": domain, \"type\": \"suspicious\", \"context\": context}\n",
        "            )\n",
        "            for domain, context in self.suspicious_domains_data\n",
        "        ]\n",
        "\n",
        "        # Create documents for trusted domains\n",
        "        trusted_docs = [\n",
        "            Document(\n",
        "                page_content=f\"{domain} - {context}\",\n",
        "                metadata={\"domain\": domain, \"type\": \"trusted\", \"context\": context}\n",
        "            )\n",
        "            for domain, context in self.trusted_domains_data\n",
        "        ]\n",
        "\n",
        "        # Create FAISS vector stores\n",
        "        self.suspicious_vectorstore = FAISS.from_documents(\n",
        "            suspicious_docs, self.embeddings\n",
        "        )\n",
        "        self.trusted_vectorstore = FAISS.from_documents(\n",
        "            trusted_docs, self.embeddings\n",
        "        )\n",
        "\n",
        "    async def get_similar_domains(\n",
        "        self,\n",
        "        query_domain: str,\n",
        "        top_k: int = 5\n",
        "    ) -> Tuple[List[Document], List[Document]]:\n",
        "        \"\"\"\n",
        "        Asynchronously retrieve the most similar suspicious and trusted domains.\n",
        "        Wraps synchronous FAISS operations in executor for non-blocking behaviour.\n",
        "\n",
        "        Args:\n",
        "            query_domain: Domain to assess\n",
        "            top_k: Number of similar examples to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (suspicious_examples, trusted_examples)\n",
        "        \"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "\n",
        "        # Run both similarity searches concurrently\n",
        "        # This is more efficient than sequential searches\n",
        "        suspicious_task = loop.run_in_executor(\n",
        "            self.executor,\n",
        "            self.suspicious_vectorstore.similarity_search,\n",
        "            query_domain,\n",
        "            top_k\n",
        "        )\n",
        "\n",
        "        trusted_task = loop.run_in_executor(\n",
        "            self.executor,\n",
        "            self.trusted_vectorstore.similarity_search,\n",
        "            query_domain,\n",
        "            top_k\n",
        "        )\n",
        "\n",
        "        # Wait for both searches to complete concurrently\n",
        "        suspicious_similar, trusted_similar = await asyncio.gather(\n",
        "            suspicious_task,\n",
        "            trusted_task\n",
        "        )\n",
        "\n",
        "        return suspicious_similar, trusted_similar\n",
        "\n",
        "    async def create_assessment_prompt(\n",
        "        self,\n",
        "        domain: str,\n",
        "        email_context: dict,\n",
        "        top_k: int = 5\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Asynchronously create an enhanced prompt with similar domain examples.\n",
        "        Uses async similarity search for better performance.\n",
        "\n",
        "        Args:\n",
        "            domain: Domain to assess\n",
        "            email_context: Email context for assessment\n",
        "            top_k: Number of examples to include\n",
        "\n",
        "        Returns:\n",
        "            Enhanced prompt with examples\n",
        "        \"\"\"\n",
        "        # Get similar domains asynchronously\n",
        "        suspicious_examples, trusted_examples = await self.get_similar_domains(domain, top_k)\n",
        "\n",
        "        # Format examples (synchronous string manipulation)\n",
        "        suspicious_text = \"\\n\".join([\n",
        "            f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\"\n",
        "            for doc in suspicious_examples\n",
        "        ])\n",
        "\n",
        "        trusted_text = \"\\n\".join([\n",
        "            f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\"\n",
        "            for doc in trusted_examples\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"Analyse this email sender domain for security risks using the provided examples.\n",
        "\n",
        "Domain to assess: {domain}\n",
        "Email Subject: {email_context.get('subject', 'N/A')}\n",
        "Sender Full Address: {email_context.get('from', 'N/A')}\n",
        "\n",
        "SIMILAR SUSPICIOUS DOMAINS (for reference):\n",
        "{suspicious_text}\n",
        "\n",
        "SIMILAR TRUSTED DOMAINS (for reference):\n",
        "{trusted_text}\n",
        "\n",
        "Based on:\n",
        "1. The similarity to the suspicious examples above\n",
        "2. The similarity to the trusted examples above\n",
        "3. Common phishing patterns (typosquatting, brand impersonation, etc.)\n",
        "4. The email context provided\n",
        "\n",
        "Assess if this domain appears:\n",
        "- SUSPICIOUS (likely phishing/scam)\n",
        "- TRUSTED (legitimate business/service)\n",
        "- UNKNOWN (cannot determine with confidence)\n",
        "\n",
        "Consider:\n",
        "- Does it closely resemble any suspicious examples?\n",
        "- Does it match patterns from trusted examples?\n",
        "- Are there spelling variations of known brands?\n",
        "- Does it use suspicious subdomain patterns?\n",
        "\n",
        "Respond with ONLY one word: SUSPICIOUS, TRUSTED, or UNKNOWN\n",
        "\n",
        "Decision:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup method to properly shutdown the thread pool executor.\"\"\"\n",
        "        if hasattr(self, 'executor'):\n",
        "            self.executor.shutdown(wait=False)\n",
        "\n",
        "\n",
        "# Integration with AsyncSecurityAnalyser\n",
        "def integrate_async_similarity_matcher(async_security_analyser_class):\n",
        "    \"\"\"\n",
        "    Monkey-patch the AsyncSecurityAnalyser to use similarity matching.\n",
        "    This enhances the domain assessment with vector similarity search.\n",
        "    In production, this should be properly integrated into the class.\n",
        "\n",
        "    Args:\n",
        "        async_security_analyser_class: The AsyncSecurityAnalyser class to enhance\n",
        "\n",
        "    Returns:\n",
        "        AsyncDomainSimilarityMatcher instance\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise the async matcher\n",
        "    domain_matcher = AsyncDomainSimilarityMatcher()\n",
        "\n",
        "    # Store original method\n",
        "    original_assess = async_security_analyser_class.assess_domain_with_llm\n",
        "\n",
        "    async def enhanced_assess_domain(self, domain: str, email_context: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Enhanced async domain assessment using similarity matching.\n",
        "        Combines vector similarity with LLM assessment for better accuracy.\n",
        "\n",
        "        Args:\n",
        "            domain: Domain to assess\n",
        "            email_context: Email context\n",
        "\n",
        "        Returns:\n",
        "            Assessment results dictionary\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Create enhanced prompt with examples asynchronously\n",
        "            prompt = await domain_matcher.create_assessment_prompt(\n",
        "                domain, email_context, top_k=5\n",
        "            )\n",
        "\n",
        "            # Get LLM assessment asynchronously\n",
        "            response = await self.security_llm.ainvoke(prompt)\n",
        "            assessment = response.content.strip().upper()\n",
        "\n",
        "            # Validate response\n",
        "            if assessment not in ['SUSPICIOUS', 'TRUSTED', 'UNKNOWN']:\n",
        "                assessment = 'UNKNOWN'\n",
        "\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': assessment,\n",
        "                'confidence': 'high' if assessment != 'UNKNOWN' else 'medium',\n",
        "                'method': 'similarity_enhanced'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced assessment failed for {domain}: {e}\")\n",
        "            # Fall back to original method\n",
        "            return await original_assess(self, domain, email_context)\n",
        "\n",
        "    # Replace method with async version\n",
        "    async_security_analyser_class.assess_domain_with_llm = enhanced_assess_domain\n",
        "\n",
        "    return domain_matcher\n",
        "\n",
        "\n",
        "# Test function for the async domain matcher\n",
        "async def test_async_domain_matcher():\n",
        "    \"\"\"Test the AsyncDomainSimilarityMatcher with sample domains.\"\"\"\n",
        "    matcher = AsyncDomainSimilarityMatcher()\n",
        "\n",
        "    # Test domain similarity search\n",
        "    test_domain = \"arnazon.com\"  # Note the typo (mimicking Amazon)\n",
        "    suspicious, trusted = await matcher.get_similar_domains(test_domain)\n",
        "\n",
        "    print(\"‚úÖ Async Domain Matcher Test Results:\")\n",
        "    print(f\"\\nTesting domain: {test_domain}\")\n",
        "    print(f\"\\nTop suspicious similar domains:\")\n",
        "    for doc in suspicious[:3]:\n",
        "        print(f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\")\n",
        "\n",
        "    print(f\"\\nTop trusted similar domains:\")\n",
        "    for doc in trusted[:3]:\n",
        "        print(f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\")\n",
        "\n",
        "    return matcher\n",
        "\n",
        "\n",
        "# Execute the test\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        domain_matcher = asyncio.run(test_async_domain_matcher())\n",
        "    except:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        domain_matcher = loop.run_until_complete(test_async_domain_matcher())\n",
        "\n",
        "print(\"‚úÖ Async Domain similarity matcher created\")"
      ],
      "metadata": {
        "id": "9x2x2lVepUpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain Agent Tools**"
      ],
      "metadata": {
        "id": "zCFGmafDlHEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Enhanced Tools with @tool Decorator and Full Content Processing - ASYNC VERSION\n",
        "\n",
        "# Define input schemas for tools - These remain the same for async version\n",
        "class EmailSummaryInput(BaseModel):\n",
        "    \"\"\"Input schema for email summarisation.\"\"\"\n",
        "    email_id: str = Field(description=\"Email ID to summarise\")\n",
        "    subject: str = Field(description=\"Email subject\")\n",
        "    sender: str = Field(description=\"Email sender\")\n",
        "    body_text: str = Field(description=\"Full email body text\")\n",
        "    body_html: Optional[str] = Field(description=\"Full email HTML content if available\")\n",
        "\n",
        "class EmailActionInput(BaseModel):\n",
        "    \"\"\"Input schema for action extraction.\"\"\"\n",
        "    email_id: str = Field(description=\"Email ID\")\n",
        "    subject: str = Field(description=\"Email subject\")\n",
        "    body_text: str = Field(description=\"Full email body text\")\n",
        "    sender: str = Field(description=\"Email sender\")\n",
        "\n",
        "class EmailSecurityInput(BaseModel):\n",
        "    \"\"\"Input schema for security analysis.\"\"\"\n",
        "    email_data: Dict[str, Any] = Field(description=\"Complete email data dictionary\")\n",
        "\n",
        "# Create LLM for summarisation (separate from security)\n",
        "# Note: Same LLM instance works for both sync and async operations\n",
        "summarisation_llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.3  # Slightly higher for more natural summaries\n",
        ")\n",
        "\n",
        "@tool(\"email_summariser\", args_schema=EmailSummaryInput, return_direct=False)\n",
        "async def summarise_email(\n",
        "    email_id: str,\n",
        "    subject: str,\n",
        "    sender: str,\n",
        "    body_text: str,\n",
        "    body_html: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Provide comprehensive email summarisation using full content.\n",
        "    This async tool reads the ENTIRE email content to create proper summaries.\n",
        "    Uses async LLM invocation for non-blocking operation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use full body text, not regex extracts\n",
        "    full_content = body_text if body_text else \"\"\n",
        "\n",
        "    # If HTML is available and text is empty, extract text from HTML\n",
        "    if not full_content and body_html:\n",
        "        # Basic HTML tag removal (in production, use BeautifulSoup)\n",
        "        import re\n",
        "        full_content = re.sub('<[^<]+?>', '', body_html)\n",
        "\n",
        "    # Create comprehensive summarisation prompt\n",
        "    prompt = f\"\"\"Provide a comprehensive summary of this email.\n",
        "\n",
        "Email Details:\n",
        "- Subject: {subject}\n",
        "- From: {sender}\n",
        "- Email ID: {email_id}\n",
        "\n",
        "Full Email Content:\n",
        "{full_content[:5000]}  # Limit to 5000 chars for token management\n",
        "\n",
        "Create a detailed summary that includes:\n",
        "1. **Main Purpose**: What is the primary reason for this email?\n",
        "2. **Key Points**: List the most important information (3-5 points)\n",
        "3. **Context**: Any relevant background or context mentioned\n",
        "4. **Tone**: Professional, casual, urgent, informative, etc.\n",
        "5. **Important Details**: Dates, numbers, names, specific requirements\n",
        "6. **Attachments Mentioned**: Any files or documents referenced\n",
        "\n",
        "Provide a clear, comprehensive summary that would allow someone to understand\n",
        "the email without reading it. Focus on clarity and completeness.\n",
        "\n",
        "Format your response as JSON with the following structure:\n",
        "{{\n",
        "    \"main_purpose\": \"...\",\n",
        "    \"key_points\": [\"point1\", \"point2\", ...],\n",
        "    \"context\": \"...\",\n",
        "    \"tone\": \"...\",\n",
        "    \"important_details\": {{\n",
        "        \"dates\": [...],\n",
        "        \"numbers\": [...],\n",
        "        \"names\": [...],\n",
        "        \"requirements\": [...]\n",
        "    }},\n",
        "    \"attachments_mentioned\": [...],\n",
        "    \"executive_summary\": \"A 2-3 sentence overview\",\n",
        "    \"word_count\": <approximate word count of original email>\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use async invocation for non-blocking LLM call\n",
        "        response = await summarisation_llm.ainvoke(prompt)\n",
        "\n",
        "        # Parse JSON response\n",
        "        content = response.content\n",
        "        # Clean potential markdown formatting\n",
        "        if \"```json\" in content:\n",
        "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "        elif \"```\" in content:\n",
        "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "\n",
        "        summary_data = json.loads(content.strip())\n",
        "\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"sender\": sender,\n",
        "            \"summary\": summary_data,\n",
        "            \"content_length\": len(full_content),\n",
        "            \"summarisation_successful\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Summarisation failed: {e}\")\n",
        "        # Fallback to basic summary\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"sender\": sender,\n",
        "            \"summary\": {\n",
        "                \"executive_summary\": f\"Email from {sender} about: {subject}\",\n",
        "                \"error\": str(e)\n",
        "            },\n",
        "            \"content_length\": len(full_content),\n",
        "            \"summarisation_successful\": False\n",
        "        }\n",
        "\n",
        "@tool(\"extract_actions\", args_schema=EmailActionInput, return_direct=False)\n",
        "async def extract_action_items(\n",
        "    email_id: str,\n",
        "    subject: str,\n",
        "    body_text: str,\n",
        "    sender: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract action items and to-dos from email using full content analysis.\n",
        "    Uses async LLM to understand context and extract meaningful actions.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Extract all action items, tasks, and deadlines from this email.\n",
        "\n",
        "Email Subject: {subject}\n",
        "From: {sender}\n",
        "\n",
        "Full Email Content:\n",
        "{body_text[:5000]}\n",
        "\n",
        "Identify and extract:\n",
        "1. **Direct Requests**: Things explicitly asked to be done\n",
        "2. **Implied Tasks**: Actions implied but not directly stated\n",
        "3. **Deadlines**: Any time-sensitive items with dates\n",
        "4. **Follow-ups**: Items requiring response or follow-up\n",
        "5. **Decisions Required**: Points needing decisions\n",
        "6. **Information Requests**: Requests for information or documents\n",
        "\n",
        "For each action item, specify:\n",
        "- The specific action required\n",
        "- Who needs to do it (if mentioned)\n",
        "- Deadline or timeframe (if any)\n",
        "- Priority level (High/Medium/Low based on context)\n",
        "- Category (Request/Deadline/Follow-up/Decision/Information)\n",
        "\n",
        "Format as JSON:\n",
        "{{\n",
        "    \"action_items\": [\n",
        "        {{\n",
        "            \"action\": \"...\",\n",
        "            \"assigned_to\": \"recipient/specific person/not specified\",\n",
        "            \"deadline\": \"date or null\",\n",
        "            \"priority\": \"High/Medium/Low\",\n",
        "            \"category\": \"...\"\n",
        "        }}\n",
        "    ],\n",
        "    \"total_actions\": <number>,\n",
        "    \"has_urgent_items\": true/false,\n",
        "    \"summary_of_requirements\": \"Brief overview of what's needed\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use async invocation for non-blocking LLM call\n",
        "        response = await summarisation_llm.ainvoke(prompt)\n",
        "\n",
        "        # Parse response\n",
        "        content = response.content\n",
        "        if \"```json\" in content:\n",
        "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "        elif \"```\" in content:\n",
        "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "\n",
        "        actions_data = json.loads(content.strip())\n",
        "\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"actions\": actions_data,\n",
        "            \"extraction_successful\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Action extraction failed: {e}\")\n",
        "        return {\n",
        "            \"email_id\": email_id,\n",
        "            \"subject\": subject,\n",
        "            \"actions\": {\n",
        "                \"action_items\": [],\n",
        "                \"total_actions\": 0,\n",
        "                \"error\": str(e)\n",
        "            },\n",
        "            \"extraction_successful\": False\n",
        "        }\n",
        "\n",
        "@tool(\"security_analysis\", args_schema=EmailSecurityInput, return_direct=False)\n",
        "async def analyse_email_security_tool(email_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Perform comprehensive security analysis on email.\n",
        "    This async tool wrapper integrates with AsyncSecurityAnalyser.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if AsyncSecurityAnalyser is available in globals\n",
        "    # In production, this would be properly injected\n",
        "    if 'security_analyser' in globals():\n",
        "        # Use the global async security analyser instance\n",
        "        security_result = await security_analyser.analyse_email_security(email_data)\n",
        "        return security_result\n",
        "    else:\n",
        "        # Fallback if analyser not initialised\n",
        "        # In production, ensure proper initialisation\n",
        "        print(\"‚ö†Ô∏è Security analyser not initialised, creating temporary instance\")\n",
        "\n",
        "        # Create a temporary instance for this analysis\n",
        "        from typing import TYPE_CHECKING\n",
        "        if not TYPE_CHECKING:\n",
        "            temp_analyser = AsyncSecurityAnalyser()\n",
        "            security_result = await temp_analyser.analyse_email_security(email_data)\n",
        "            return security_result\n",
        "\n",
        "        # Default response if analyser unavailable\n",
        "        return {\n",
        "            \"email_id\": email_data.get(\"id\"),\n",
        "            \"risk_assessment\": {\n",
        "                \"overall_risk\": \"unknown\",\n",
        "                \"risk_score\": 0,\n",
        "                \"risk_factors\": [],\n",
        "                \"error\": \"Security analyser not properly initialised\"\n",
        "            },\n",
        "            \"recommendations\": [\"Security analysis unavailable\"],\n",
        "            \"security_check_complete\": False\n",
        "        }\n",
        "\n",
        "# Async tool registration function for LangChain\n",
        "def get_async_enhanced_tools():\n",
        "    \"\"\"\n",
        "    Get all enhanced async tools with proper structure.\n",
        "    These async tools will show up correctly in LangSmith traces\n",
        "    and work with async agents.\n",
        "\n",
        "    Returns:\n",
        "        List of async tool functions\n",
        "    \"\"\"\n",
        "    return [\n",
        "        summarise_email,\n",
        "        extract_action_items,\n",
        "        analyse_email_security_tool\n",
        "    ]\n",
        "\n",
        "# Helper function for proper tool invocation in agents\n",
        "async def invoke_tool_async(tool, input_data):\n",
        "    \"\"\"\n",
        "    Helper function to properly invoke async tools.\n",
        "\n",
        "    Args:\n",
        "        tool: The tool decorated with @tool\n",
        "        input_data: Dictionary of input parameters\n",
        "\n",
        "    Returns:\n",
        "        Tool execution result\n",
        "\n",
        "    Example:\n",
        "        result = await invoke_tool_async(summarise_email, {\n",
        "            \"email_id\": \"123\",\n",
        "            \"subject\": \"Test\",\n",
        "            \"sender\": \"test@example.com\",\n",
        "            \"body_text\": \"Content...\"\n",
        "        })\n",
        "    \"\"\"\n",
        "    return await tool.ainvoke(input_data)\n",
        "\n",
        "# Helper function to create tools with custom security analyser\n",
        "def create_async_tools_with_analyser(analyser_instance):\n",
        "    \"\"\"\n",
        "    Create async tools with a specific security analyser instance.\n",
        "    This allows proper dependency injection in production.\n",
        "\n",
        "    Args:\n",
        "        analyser_instance: AsyncSecurityAnalyser instance to use\n",
        "\n",
        "    Returns:\n",
        "        List of async tools configured with the analyser\n",
        "    \"\"\"\n",
        "\n",
        "    @tool(\"security_analysis_configured\", args_schema=EmailSecurityInput, return_direct=False)\n",
        "    async def analyse_email_security_configured(email_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform comprehensive security analysis on email.\n",
        "        Uses the provided analyser instance.\n",
        "        \"\"\"\n",
        "        security_result = await analyser_instance.analyse_email_security(email_data)\n",
        "        return security_result\n",
        "\n",
        "    # Return all tools including the configured security tool\n",
        "    # Note: These tools must be invoked using .ainvoke() method\n",
        "    return [\n",
        "        summarise_email,\n",
        "        extract_action_items,\n",
        "        analyse_email_security_configured\n",
        "    ]\n",
        "\n",
        "# IMPORTANT NOTE:\n",
        "# When using @tool decorator, the functions become Tool objects.\n",
        "# They must be invoked using .ainvoke() method (for async) or .invoke() (for sync)\n",
        "# NOT called directly as functions.\n",
        "\n",
        "# Test function for async tools\n",
        "async def test_async_tools():\n",
        "    \"\"\"Test the async tools with sample data.\"\"\"\n",
        "\n",
        "    # Test summarisation tool using ainvoke method for tools\n",
        "    test_summary = await summarise_email.ainvoke({\n",
        "        \"email_id\": \"test123\",\n",
        "        \"subject\": \"Test Email\",\n",
        "        \"sender\": \"test@example.com\",\n",
        "        \"body_text\": \"This is a test email with some content to summarise.\",\n",
        "        \"body_html\": None\n",
        "    })\n",
        "    print(f\"‚úÖ Summarisation tool test: {'successful' if test_summary['summarisation_successful'] else 'failed'}\")\n",
        "\n",
        "    # Test action extraction tool using ainvoke method\n",
        "    test_actions = await extract_action_items.ainvoke({\n",
        "        \"email_id\": \"test123\",\n",
        "        \"subject\": \"Action Required\",\n",
        "        \"body_text\": \"Please review the document by Friday and send feedback.\",\n",
        "        \"sender\": \"boss@example.com\"\n",
        "    })\n",
        "    print(f\"‚úÖ Action extraction tool test: {'successful' if test_actions['extraction_successful'] else 'failed'}\")\n",
        "\n",
        "    print(\"‚úÖ All async tools tested\")\n",
        "\n",
        "# Execute the test\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        asyncio.run(test_async_tools())\n",
        "    except:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        loop.run_until_complete(test_async_tools())\n",
        "\n",
        "print(\"‚úÖ Enhanced async tools with @tool decorator created\")"
      ],
      "metadata": {
        "id": "3vfLoDGOpb5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main Agent Configuration**"
      ],
      "metadata": {
        "id": "zPBZA03yTQAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Enhanced Email Agent with Proper Integration - ASYNC VERSION\n",
        "\n",
        "class AsyncEnhancedAgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Enhanced state for the async email agent.\n",
        "    Note: State structure remains the same for async version.\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    user_query: str\n",
        "    parsed_intent: Dict[str, Any]\n",
        "    email_data: Dict[str, Any]\n",
        "    summaries: List[Dict[str, Any]]\n",
        "    actions: List[Dict[str, Any]]\n",
        "    security_results: List[Dict[str, Any]]\n",
        "    final_output: str\n",
        "\n",
        "\n",
        "class AsyncEnhancedEmailAgent:\n",
        "    \"\"\"\n",
        "    Asynchronous enhanced email management agent with proper tool integration,\n",
        "    dynamic query parsing, and comprehensive summarisation.\n",
        "    Uses async/await patterns throughout for improved performance and reduced latency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"gpt-4o-mini\",\n",
        "        temperature: float = 0.3,\n",
        "        use_similarity_matching: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the enhanced async agent.\n",
        "        Note: Initialisation remains synchronous as it's one-time setup.\n",
        "\n",
        "        Args:\n",
        "            model_name: LLM model to use\n",
        "            temperature: Model temperature\n",
        "            use_similarity_matching: Whether to use domain similarity matching\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialise LLM\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
        "\n",
        "        # Initialise async components\n",
        "        self.email_fetcher = AsyncEmailFetcher()\n",
        "        self.query_parser = AsyncIntelligentQueryParser()\n",
        "\n",
        "        # Initialise enhanced async security analyser\n",
        "        self.security_analyser = AsyncSecurityAnalyser()\n",
        "        if use_similarity_matching:\n",
        "            self.domain_matcher = integrate_async_similarity_matcher(AsyncSecurityAnalyser)\n",
        "\n",
        "        # Get enhanced async tools\n",
        "        self.tools = get_async_enhanced_tools()\n",
        "\n",
        "        # Build the enhanced async graph\n",
        "        self.app = self._build_enhanced_graph()\n",
        "\n",
        "    def _build_enhanced_graph(self) -> StateGraph:\n",
        "        \"\"\"\n",
        "        Build the enhanced async LangGraph workflow.\n",
        "        All nodes are now async functions for non-blocking execution.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create workflow with async state\n",
        "        workflow = StateGraph(AsyncEnhancedAgentState)\n",
        "\n",
        "        # Add async nodes\n",
        "        workflow.add_node(\"parse_query\", self._parse_query)\n",
        "        workflow.add_node(\"fetch_emails\", self._fetch_emails)\n",
        "        workflow.add_node(\"security_check\", self._security_analysis)\n",
        "        workflow.add_node(\"summarise_emails\", self._summarise_emails)\n",
        "        workflow.add_node(\"extract_actions\", self._extract_actions)\n",
        "        workflow.add_node(\"generate_output\", self._generate_final_output)\n",
        "\n",
        "        # Define the flow (same as sync version)\n",
        "        workflow.set_entry_point(\"parse_query\")\n",
        "        workflow.add_edge(\"parse_query\", \"fetch_emails\")\n",
        "        workflow.add_edge(\"fetch_emails\", \"security_check\")\n",
        "        workflow.add_edge(\"security_check\", \"summarise_emails\")\n",
        "        workflow.add_edge(\"summarise_emails\", \"extract_actions\")\n",
        "        workflow.add_edge(\"extract_actions\", \"generate_output\")\n",
        "        workflow.add_edge(\"generate_output\", END)\n",
        "\n",
        "        # Compile for async execution\n",
        "        return workflow.compile()\n",
        "\n",
        "    async def _parse_query(self, state: AsyncEnhancedAgentState) -> AsyncEnhancedAgentState:\n",
        "        \"\"\"\n",
        "        Asynchronously parse user query to understand intent.\n",
        "        Uses async query parser for non-blocking operation.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üß† Parsing user query with intelligence (async)...\")\n",
        "\n",
        "        user_query = state.get(\"user_query\", \"\")\n",
        "\n",
        "        # Parse the query asynchronously\n",
        "        intent = await self.query_parser.parse_user_query(user_query)\n",
        "        state[\"parsed_intent\"] = intent.model_dump()\n",
        "\n",
        "        print(f\"  ‚úÖ Intent parsed: {intent.model_dump()}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    async def _fetch_emails(self, state: AsyncEnhancedAgentState) -> AsyncEnhancedAgentState:\n",
        "        \"\"\"\n",
        "        Asynchronously fetch emails based on parsed intent.\n",
        "        Uses async email fetcher for concurrent email retrieval.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üìß Fetching emails based on parsed intent (async)...\")\n",
        "\n",
        "        intent = EmailQueryIntent(**state[\"parsed_intent\"])\n",
        "\n",
        "        # Build Gmail query (synchronous as it's just string manipulation)\n",
        "        gmail_query, max_results = self.query_parser.build_gmail_query(intent)\n",
        "\n",
        "        print(f\"  Query: '{gmail_query}' (max: {max_results})\")\n",
        "\n",
        "        try:\n",
        "            # Fetch emails asynchronously\n",
        "            df = await self.email_fetcher.fetch_emails(gmail_query, max_results)\n",
        "\n",
        "            if not df.empty:\n",
        "                emails = df.to_dict('records')\n",
        "                state[\"email_data\"] = {\n",
        "                    \"emails\": emails,\n",
        "                    \"count\": len(emails),\n",
        "                    \"query\": gmail_query\n",
        "                }\n",
        "                print(f\"  ‚úÖ Fetched {len(emails)} emails asynchronously\")\n",
        "            else:\n",
        "                state[\"email_data\"] = {\"emails\": [], \"count\": 0}\n",
        "                print(\"  ‚ö†Ô∏è No emails found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            state[\"email_data\"] = {\"error\": str(e)}\n",
        "            print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    async def _security_analysis(self, state: AsyncEnhancedAgentState) -> AsyncEnhancedAgentState:\n",
        "        \"\"\"\n",
        "        Perform async security analysis on all emails.\n",
        "        Uses concurrent processing for multiple email security checks.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üîí Performing security analysis (async)...\")\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "\n",
        "        if emails:\n",
        "            # Create tasks for concurrent security analysis\n",
        "            security_tasks = [\n",
        "                self.security_analyser.analyse_email_security(email)\n",
        "                for email in emails\n",
        "            ]\n",
        "\n",
        "            # Run all security analyses concurrently\n",
        "            security_results = await asyncio.gather(*security_tasks, return_exceptions=True)\n",
        "\n",
        "            # Handle any exceptions and filter results\n",
        "            valid_results = []\n",
        "            for result in security_results:\n",
        "                if isinstance(result, dict):\n",
        "                    valid_results.append(result)\n",
        "                elif isinstance(result, Exception):\n",
        "                    print(f\"  ‚ö†Ô∏è Security analysis error: {result}\")\n",
        "                    valid_results.append({\n",
        "                        \"overall_risk_level\": \"unknown\",\n",
        "                        \"error\": str(result)\n",
        "                    })\n",
        "\n",
        "            state[\"security_results\"] = valid_results\n",
        "\n",
        "            high_risk = sum(1 for r in valid_results\n",
        "                           if r.get(\"overall_risk_level\") in [\"high\", \"critical\"])\n",
        "            print(f\"  ‚úÖ Security check complete: {high_risk} high-risk emails\")\n",
        "        else:\n",
        "            state[\"security_results\"] = []\n",
        "            print(\"  ‚ö†Ô∏è No emails to analyse\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    async def _summarise_emails(self, state: AsyncEnhancedAgentState) -> AsyncEnhancedAgentState:\n",
        "        \"\"\"\n",
        "        Create comprehensive summaries using full email content.\n",
        "        Uses concurrent async tool invocations for better performance.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üìù Creating comprehensive summaries (async)...\")\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "\n",
        "        if emails:\n",
        "            # Create tasks for concurrent summarisation\n",
        "            summary_tasks = []\n",
        "            for email in emails:\n",
        "                # Use tool.ainvoke() for async tool invocation\n",
        "                task = summarise_email.ainvoke({\n",
        "                    \"email_id\": email.get(\"id\"),\n",
        "                    \"subject\": email.get(\"subject\"),\n",
        "                    \"sender\": email.get(\"from\"),\n",
        "                    \"body_text\": email.get(\"body_text\", \"\"),  # FULL text\n",
        "                    \"body_html\": email.get(\"body_html\", \"\")   # FULL HTML\n",
        "                })\n",
        "                summary_tasks.append(task)\n",
        "\n",
        "            # Run all summarisations concurrently\n",
        "            summaries = await asyncio.gather(*summary_tasks, return_exceptions=True)\n",
        "\n",
        "            # Handle any exceptions\n",
        "            valid_summaries = []\n",
        "            for summary in summaries:\n",
        "                if isinstance(summary, dict):\n",
        "                    valid_summaries.append(summary)\n",
        "                elif isinstance(summary, Exception):\n",
        "                    print(f\"  ‚ö†Ô∏è Summarisation error: {summary}\")\n",
        "                    valid_summaries.append({\n",
        "                        \"summarisation_successful\": False,\n",
        "                        \"error\": str(summary)\n",
        "                    })\n",
        "\n",
        "            state[\"summaries\"] = valid_summaries\n",
        "            print(f\"  ‚úÖ Created {len(valid_summaries)} comprehensive summaries\")\n",
        "        else:\n",
        "            state[\"summaries\"] = []\n",
        "            print(\"  ‚ö†Ô∏è No emails to summarise\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    async def _extract_actions(self, state: AsyncEnhancedAgentState) -> AsyncEnhancedAgentState:\n",
        "        \"\"\"\n",
        "        Extract action items from emails asynchronously.\n",
        "        Uses concurrent processing for multiple emails.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üìã Extracting action items (async)...\")\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "\n",
        "        if emails:\n",
        "            # Create tasks for concurrent action extraction\n",
        "            action_tasks = []\n",
        "            for email in emails:\n",
        "                # Use tool.ainvoke() for async tool invocation\n",
        "                task = extract_action_items.ainvoke({\n",
        "                    \"email_id\": email.get(\"id\"),\n",
        "                    \"subject\": email.get(\"subject\"),\n",
        "                    \"body_text\": email.get(\"body_text\", \"\"),  # FULL text\n",
        "                    \"sender\": email.get(\"from\")\n",
        "                })\n",
        "                action_tasks.append(task)\n",
        "\n",
        "            # Run all action extractions concurrently\n",
        "            actions = await asyncio.gather(*action_tasks, return_exceptions=True)\n",
        "\n",
        "            # Handle any exceptions\n",
        "            valid_actions = []\n",
        "            for action in actions:\n",
        "                if isinstance(action, dict):\n",
        "                    valid_actions.append(action)\n",
        "                elif isinstance(action, Exception):\n",
        "                    print(f\"  ‚ö†Ô∏è Action extraction error: {action}\")\n",
        "                    valid_actions.append({\n",
        "                        \"extraction_successful\": False,\n",
        "                        \"actions\": {\"total_actions\": 0},\n",
        "                        \"error\": str(action)\n",
        "                    })\n",
        "\n",
        "            state[\"actions\"] = valid_actions\n",
        "\n",
        "            total_actions = sum(\n",
        "                a.get(\"actions\", {}).get(\"total_actions\", 0)\n",
        "                for a in valid_actions\n",
        "            )\n",
        "            print(f\"  ‚úÖ Extracted {total_actions} total action items\")\n",
        "        else:\n",
        "            state[\"actions\"] = []\n",
        "            print(\"  ‚ö†Ô∏è No emails to extract actions from\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    async def _generate_final_output(self, state: AsyncEnhancedAgentState) -> AsyncEnhancedAgentState:\n",
        "        \"\"\"\n",
        "        Generate comprehensive final output with proper structure.\n",
        "        This remains mostly synchronous as it's string formatting.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üìä Generating comprehensive final report...\")\n",
        "\n",
        "        # Build structured output (synchronous string operations)\n",
        "        output = self._build_comprehensive_output(state)\n",
        "\n",
        "        state[\"final_output\"] = output\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _build_comprehensive_output(self, state: AsyncEnhancedAgentState) -> str:\n",
        "        \"\"\"\n",
        "        Build comprehensive output with:\n",
        "        1. Security warnings (if any)\n",
        "        2. Full email summaries\n",
        "        3. Consolidated action items\n",
        "        4. Clear structure and formatting\n",
        "\n",
        "        Note: This remains synchronous as it's pure string manipulation.\n",
        "        \"\"\"\n",
        "\n",
        "        emails = state.get(\"email_data\", {}).get(\"emails\", [])\n",
        "        summaries = state.get(\"summaries\", [])\n",
        "        actions = state.get(\"actions\", [])\n",
        "        security_results = state.get(\"security_results\", [])\n",
        "\n",
        "        output = \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += \"üìä COMPREHENSIVE EMAIL ANALYSIS REPORT (ASYNC)\\n\"\n",
        "        output += \"=\"*80 + \"\\n\\n\"\n",
        "\n",
        "        # Overview\n",
        "        output += f\"üìß Total Emails Analysed: {len(emails)}\\n\"\n",
        "        output += f\"üîç Query Used: {state.get('email_data', {}).get('query', 'N/A')}\\n\"\n",
        "\n",
        "        # Security Section First (Priority)\n",
        "        high_risk_emails = [\n",
        "            (i, r) for i, r in enumerate(security_results)\n",
        "            if r.get(\"overall_risk_level\") in [\"high\", \"critical\"]\n",
        "        ]\n",
        "\n",
        "        if high_risk_emails:\n",
        "            output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "            output += \"‚ö†Ô∏è SECURITY ALERTS - IMMEDIATE ATTENTION REQUIRED\\n\"\n",
        "            output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "            for idx, risk in high_risk_emails:\n",
        "                email = emails[idx]\n",
        "                output += f\"\\nüî¥ HIGH RISK: {email.get('subject', 'No subject')}\\n\"\n",
        "                output += f\"   From: {email.get('from', 'Unknown')}\\n\"\n",
        "                output += f\"   Risk Level: {risk.get('overall_risk_level', '').upper()}\\n\"\n",
        "                output += f\"   Risk Score: {risk.get('overall_risk_score', 0)}/100\\n\"\n",
        "\n",
        "                recommendations = risk.get(\"recommendations\", [])\n",
        "                if recommendations:\n",
        "                    output += \"   Recommendations:\\n\"\n",
        "                    for rec in recommendations[:3]:\n",
        "                        output += f\"     ‚Ä¢ {rec}\\n\"\n",
        "\n",
        "        # Full Email Summaries Section\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += \"üìù DETAILED EMAIL SUMMARIES\\n\"\n",
        "        output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "        for i, (email, summary, action, security) in enumerate(\n",
        "            zip(emails, summaries, actions, security_results), 1\n",
        "        ):\n",
        "            output += f\"\\n{i}. {email.get('subject', 'No subject')}\\n\"\n",
        "            output += \"-\"*60 + \"\\n\"\n",
        "            output += f\"From: {email.get('from', 'Unknown')}\\n\"\n",
        "            output += f\"Date: {str(email.get('date', 'Unknown'))[:19]}\\n\"\n",
        "            output += f\"Security Status: {security.get('overall_risk_level', 'unchecked')}\\n\"\n",
        "\n",
        "            # Comprehensive Summary\n",
        "            if summary.get(\"summarisation_successful\"):\n",
        "                sum_data = summary.get(\"summary\", {})\n",
        "                output += f\"\\nüìå Executive Summary:\\n\"\n",
        "                output += f\"   {sum_data.get('executive_summary', 'Not available')}\\n\"\n",
        "\n",
        "                output += f\"\\nüìç Main Purpose:\\n\"\n",
        "                output += f\"   {sum_data.get('main_purpose', 'Not identified')}\\n\"\n",
        "\n",
        "                key_points = sum_data.get(\"key_points\", [])\n",
        "                if key_points:\n",
        "                    output += f\"\\nüîë Key Points:\\n\"\n",
        "                    for point in key_points:\n",
        "                        output += f\"   ‚Ä¢ {point}\\n\"\n",
        "\n",
        "                important = sum_data.get(\"important_details\", {})\n",
        "                if any(important.values()):\n",
        "                    output += f\"\\nüìä Important Details:\\n\"\n",
        "                    if important.get(\"dates\"):\n",
        "                        output += f\"   Dates: {', '.join(important['dates'])}\\n\"\n",
        "                    if important.get(\"requirements\"):\n",
        "                        output += f\"   Requirements: {', '.join(important['requirements'])}\\n\"\n",
        "            else:\n",
        "                output += f\"\\n‚ö†Ô∏è Summarisation failed: {summary.get('summary', {}).get('error', 'Unknown error')}\\n\"\n",
        "\n",
        "        # Consolidated Action Items\n",
        "        all_actions = []\n",
        "        for i, action_set in enumerate(actions):\n",
        "            if action_set.get(\"extraction_successful\"):\n",
        "                for action in action_set.get(\"actions\", {}).get(\"action_items\", []):\n",
        "                    action[\"email_subject\"] = emails[i].get(\"subject\", \"\")\n",
        "                    all_actions.append(action)\n",
        "\n",
        "        if all_actions:\n",
        "            output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "            output += \"üìã CONSOLIDATED ACTION ITEMS & TO-DO LIST\\n\"\n",
        "            output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "            # Sort by priority\n",
        "            high_priority = [a for a in all_actions if a.get(\"priority\") == \"High\"]\n",
        "            medium_priority = [a for a in all_actions if a.get(\"priority\") == \"Medium\"]\n",
        "            low_priority = [a for a in all_actions if a.get(\"priority\") == \"Low\"]\n",
        "\n",
        "            if high_priority:\n",
        "                output += \"\\nüî¥ HIGH PRIORITY:\\n\"\n",
        "                for action in high_priority:\n",
        "                    output += f\"   ‚ñ° {action['action']}\\n\"\n",
        "                    if action.get(\"deadline\"):\n",
        "                        output += f\"     ‚è∞ Deadline: {action['deadline']}\\n\"\n",
        "                    output += f\"     üìß From: {action['email_subject'][:50]}...\\n\"\n",
        "\n",
        "            if medium_priority:\n",
        "                output += \"\\nüü° MEDIUM PRIORITY:\\n\"\n",
        "                for action in medium_priority:\n",
        "                    output += f\"   ‚ñ° {action['action']}\\n\"\n",
        "                    if action.get(\"deadline\"):\n",
        "                        output += f\"     ‚è∞ Deadline: {action['deadline']}\\n\"\n",
        "\n",
        "            if low_priority:\n",
        "                output += \"\\nüü¢ LOW PRIORITY:\\n\"\n",
        "                for action in low_priority[:5]:  # Limit to 5\n",
        "                    output += f\"   ‚ñ° {action['action']}\\n\"\n",
        "\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += \"‚úÖ END OF REPORT (ASYNC PROCESSING)\\n\"\n",
        "        output += \"=\"*80 + \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    async def process_emails(self, user_query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Asynchronously process user email query with enhanced capabilities.\n",
        "        Main entry point for the async agent.\n",
        "\n",
        "        Args:\n",
        "            user_query: Natural language query from user\n",
        "\n",
        "        Returns:\n",
        "            Dict with results and status\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ü§ñ ASYNC ENHANCED EMAIL AGENT ACTIVE\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüîç User Query: {user_query}\\n\")\n",
        "\n",
        "        try:\n",
        "            # Initial state\n",
        "            initial_state = {\n",
        "                \"messages\": [HumanMessage(content=user_query)],\n",
        "                \"user_query\": user_query,\n",
        "                \"parsed_intent\": {},\n",
        "                \"email_data\": {},\n",
        "                \"summaries\": [],\n",
        "                \"actions\": [],\n",
        "                \"security_results\": [],\n",
        "                \"final_output\": \"\"\n",
        "            }\n",
        "\n",
        "            # Run the async workflow\n",
        "            # Note: LangGraph's ainvoke for async execution\n",
        "            final_state = await self.app.ainvoke(initial_state)\n",
        "\n",
        "            # Display result\n",
        "            print(final_state.get(\"final_output\", \"No result\"))\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'output': final_state.get(\"final_output\"),\n",
        "                'state': final_state\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "print(\"‚úÖ AsyncEnhancedEmailAgent created with all improvements\")\n",
        "print(\"üí° Use Cell 6 to initialise and test the async agent\")"
      ],
      "metadata": {
        "id": "OqKTGJ6nww8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialise the Enhanced Agent**"
      ],
      "metadata": {
        "id": "pzhAQtI56rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Initialise the Async Enhanced Agent\n",
        "\n",
        "async def initialise_and_test_agent():\n",
        "    \"\"\"\n",
        "    Initialise the async agent and run test queries.\n",
        "    This function handles the async context properly.\n",
        "    \"\"\"\n",
        "    # Initialise the async enhanced agent\n",
        "    async_enhanced_agent = AsyncEnhancedEmailAgent(\n",
        "        model_name=\"gpt-4o-mini\",\n",
        "        temperature=0.3,\n",
        "        use_similarity_matching=True  # Enable domain similarity\n",
        "    )\n",
        "    print(\"‚úÖ Async Enhanced Email Agent initialised\")\n",
        "\n",
        "    # Test with various queries\n",
        "    test_queries = [\n",
        "        \"Show me my last 5 unread emails\",\n",
        "        \"Get all emails from today\",\n",
        "        \"Summarise last 5 important emails\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing query: '{query}'\")\n",
        "        print('='*60)\n",
        "        result = await async_enhanced_agent.process_emails(query)\n",
        "\n",
        "        if result['success']:\n",
        "            print(\"‚úÖ Query processed successfully\")\n",
        "        else:\n",
        "            print(f\"‚ùå Query failed: {result.get('error')}\")\n",
        "\n",
        "    return async_enhanced_agent\n",
        "\n",
        "# Execute the initialisation and store the agent globally\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    # Run the async function and store the result\n",
        "    async_enhanced_agent = asyncio.run(initialise_and_test_agent())\n",
        "except:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    async_enhanced_agent = loop.run_until_complete(initialise_and_test_agent())\n",
        "\n",
        "print(\"\\n‚úÖ Async agent ready for use in variable: 'async_enhanced_agent'\")"
      ],
      "metadata": {
        "id": "uNjeaHGH0fom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test**"
      ],
      "metadata": {
        "id": "koTjdqgA-Xbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Test the Async Domain Matcher\n",
        "\n",
        "async def test_domain_matcher():\n",
        "    \"\"\"\n",
        "    Test the async domain matcher with a sample domain.\n",
        "    \"\"\"\n",
        "    if 'async_enhanced_agent' in globals():\n",
        "        # The domain_matcher is stored as an attribute of the agent\n",
        "        domain_matcher = async_enhanced_agent.domain_matcher\n",
        "\n",
        "        # Now you can test it\n",
        "        test_domain = \"arnazon.com\"  # Note the typo\n",
        "\n",
        "        # Use async method to get similar domains\n",
        "        suspicious, trusted = await domain_matcher.get_similar_domains(test_domain)\n",
        "\n",
        "        print(\"‚úÖ Async Domain Matcher Test Results:\")\n",
        "        print(f\"\\nTesting domain: {test_domain}\")\n",
        "        print(f\"\\nTop suspicious similar domains:\")\n",
        "        for doc in suspicious[:5]:\n",
        "            print(f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\")\n",
        "\n",
        "        print(f\"\\nTop trusted similar domains:\")\n",
        "        for doc in trusted[:5]:\n",
        "            print(f\"  - {doc.metadata['domain']}: {doc.metadata['context']}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Async enhanced agent not yet created. Create it first with:\")\n",
        "        print(\"async_enhanced_agent = AsyncEnhancedEmailAgent(use_similarity_matching=True)\")\n",
        "\n",
        "# Execute the test\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    asyncio.run(test_domain_matcher())\n",
        "except:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(test_domain_matcher())"
      ],
      "metadata": {
        "id": "Yj4EaIbq0jQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Test Async Query Parser\n",
        "\n",
        "async def test_query_parser():\n",
        "    \"\"\"\n",
        "    Test various natural language queries with the async parser.\n",
        "    \"\"\"\n",
        "    # Create an instance of the async query parser\n",
        "    parser = AsyncIntelligentQueryParser()\n",
        "\n",
        "    # Test query\n",
        "    test_query = \"Show me urgent emails from last week\"\n",
        "\n",
        "    # Parse the query asynchronously\n",
        "    intent = await parser.parse_user_query(test_query)\n",
        "\n",
        "    print(\"‚úÖ Async Query Parser Test Results:\")\n",
        "    print(f\"Query: '{test_query}'\")\n",
        "    print(f\"Parsed intent: {intent.model_dump()}\")  # Using model_dump() as per latest Pydantic\n",
        "\n",
        "    # Build Gmail query from intent\n",
        "    gmail_query, max_results = parser.build_gmail_query(intent)\n",
        "    print(f\"Gmail query: '{gmail_query}'\")\n",
        "    print(f\"Max results: {max_results}\")\n",
        "\n",
        "    return parser\n",
        "\n",
        "# Execute the test\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    parser = asyncio.run(test_query_parser())\n",
        "except:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    parser = loop.run_until_complete(test_query_parser())"
      ],
      "metadata": {
        "id": "ICtRZspk0lYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test Complete Async Pipeline\n",
        "\n",
        "async def test_complete_pipeline():\n",
        "    \"\"\"\n",
        "    Test the complete async pipeline with various queries.\n",
        "    This demonstrates the full async processing capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if agent exists\n",
        "    if 'async_enhanced_agent' not in globals():\n",
        "        print(\"‚ö†Ô∏è Creating new async agent instance...\")\n",
        "        global async_enhanced_agent\n",
        "        async_enhanced_agent = AsyncEnhancedEmailAgent(\n",
        "            model_name=\"gpt-4o-mini\",\n",
        "            temperature=0.3,\n",
        "            use_similarity_matching=True\n",
        "        )\n",
        "\n",
        "    # Test queries for comprehensive pipeline testing\n",
        "    queries = [\n",
        "        \"Find emails from this week about meetings\",\n",
        "        # Add more test queries as needed:\n",
        "        # \"Show me all unread emails from today\",\n",
        "        # \"Get urgent emails with attachments\",\n",
        "        # \"Summarise important emails from last week\"\n",
        "    ]\n",
        "\n",
        "    print(\"üöÄ Starting Complete Async Pipeline Test\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Track timing for performance comparison\n",
        "    import time\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nüìß Processing: '{query}'\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Process the query asynchronously\n",
        "        result = await async_enhanced_agent.process_emails(query)\n",
        "\n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "\n",
        "        if result['success']:\n",
        "            print(f\"‚úÖ Success - Processed in {processing_time:.2f} seconds\")\n",
        "\n",
        "            # Extract some statistics from the result\n",
        "            state = result.get('state', {})\n",
        "            email_count = len(state.get('email_data', {}).get('emails', []))\n",
        "            action_count = sum(\n",
        "                a.get('actions', {}).get('total_actions', 0)\n",
        "                for a in state.get('actions', [])\n",
        "            )\n",
        "\n",
        "            print(f\"   üìä Statistics:\")\n",
        "            print(f\"      - Emails processed: {email_count}\")\n",
        "            print(f\"      - Actions extracted: {action_count}\")\n",
        "            print(f\"      - Processing time: {processing_time:.2f}s\")\n",
        "\n",
        "            # Calculate average time per email\n",
        "            if email_count > 0:\n",
        "                avg_time = processing_time / email_count\n",
        "                print(f\"      - Average time per email: {avg_time:.3f}s\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed: {result.get('error')}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ Complete Async Pipeline Test Finished\")\n",
        "    print(\"\\nüí° Tip: Compare these timings with your synchronous version to measure performance improvements!\")\n",
        "\n",
        "# Execute the complete pipeline test\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    asyncio.run(test_complete_pipeline())\n",
        "except:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(test_complete_pipeline())"
      ],
      "metadata": {
        "id": "85PECSjt0r3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}