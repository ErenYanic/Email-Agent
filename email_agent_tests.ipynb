{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install python-dotenv langchain langchain_community langchain-openai langsmith langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfrZVH4zruXl",
        "outputId": "ba048813-ad4c-4260-c87f-b3f72fc47476"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.4.12)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.99.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.11.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.29-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.4-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.72\n",
            "    Uninstalling langchain-core-0.3.72:\n",
            "      Successfully uninstalled langchain-core-0.3.72\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-core-0.3.74 langchain-openai-0.3.29 langchain_community-0.3.27 langgraph-0.6.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.0 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.10.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Core imports and configuration**"
      ],
      "metadata": {
        "id": "RBwm4_L0aDgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Core imports and configuration\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import Tool, BaseTool\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
        "from langchain.schema import BaseMessage, HumanMessage, SystemMessage\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Email processing imports\n",
        "import base64\n",
        "from dateutil.parser import parse\n",
        "from langchain_community.agent_toolkits import GmailToolkit\n",
        "from langchain_community.tools.gmail.utils import build_resource_service\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure environment variables\n",
        "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
        "\n",
        "# Get the OpenAI API Key from the .env file\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "else:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in .env file!\")\n",
        "\n",
        "# To test the LangSmith connection\n",
        "try:\n",
        "    from langsmith import Client\n",
        "    client = Client()\n",
        "    print(\"LangSmith connection successful!\")\n",
        "    print(f\"Proje: {os.environ.get('LANGSMITH_PROJECT')}\")\n",
        "except Exception as e:\n",
        "    print(f\"LangSmith connection error: {e}\")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "llm = init_chat_model(\"gpt-5-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Send test message (to check tracing)\n",
        "try:\n",
        "    response = llm.invoke(\"Hi, this is a test message!\")\n",
        "    print(\"LLM test successful!\")\n",
        "    print(\"You can check the traces in LangSmith.\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"LLM test error: {e}\")\n",
        "\n",
        "print(\"‚úÖ Core configuration completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6GhQ9aVVyrP",
        "outputId": "eb3e5bad-ce7e-479e-b5d1-5aa372e745c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangSmith connection successful!\n",
            "Proje: pr-husky-duster-52\n",
            "LLM test successful!\n",
            "You can check the traces in LangSmith.\n",
            "content='Hello ‚Äî got it! Test message received. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 14, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C36Z9f1nq6lpSJVnKicrJbLMw2oan', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7628a244-add4-45b9-9450-8940e59b8480-0' usage_metadata={'input_tokens': 14, 'output_tokens': 89, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}\n",
            "‚úÖ Core configuration completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Email Fetcher Class**"
      ],
      "metadata": {
        "id": "sTDzUW86aqyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Email Fetcher class - structured approach for email retrieval\n",
        "class EmailFetcher:\n",
        "    \"\"\"Gmail API email fetcher and processor class\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialise Gmail API service\"\"\"\n",
        "        self.api_resource = build_resource_service()\n",
        "        self.toolkit = GmailToolkit(api_resource=self.api_resource)\n",
        "        self.search_tool = next(\n",
        "            (tool for tool in self.toolkit.get_tools() if tool.name == 'search_gmail'),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if not self.search_tool:\n",
        "            raise ValueError(\"Gmail search tool could not be initialised!\")\n",
        "\n",
        "    def get_email_contents(self, payload: Dict) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Recursively extract content from email payload\n",
        "\n",
        "        Args:\n",
        "            payload: Email payload from Gmail API\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing 'text' and 'html' content\n",
        "        \"\"\"\n",
        "        plain_text = \"\"\n",
        "        html_text = \"\"\n",
        "\n",
        "        # Recursively process parts if they exist\n",
        "        if 'parts' in payload:\n",
        "            for part in payload['parts']:\n",
        "                nested_content = self.get_email_contents(part)\n",
        "                plain_text += nested_content['text']\n",
        "                html_text += nested_content['html']\n",
        "\n",
        "        # Decode body if present\n",
        "        elif 'body' in payload and 'data' in payload['body']:\n",
        "            mime_type = payload.get('mimeType', '')\n",
        "            body_data = payload['body'].get('data', '')\n",
        "\n",
        "            if body_data:\n",
        "                try:\n",
        "                    decoded_body = base64.urlsafe_b64decode(body_data).decode('utf-8', errors='ignore')\n",
        "                    if 'text/plain' in mime_type:\n",
        "                        plain_text += decoded_body\n",
        "                    elif 'text/html' in mime_type:\n",
        "                        html_text += decoded_body\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Decode error: {e}\")\n",
        "                    pass\n",
        "\n",
        "        return {'text': plain_text.strip(), 'html': html_text.strip()}\n",
        "\n",
        "    def fetch_emails(self, query: str = \"in:inbox\", max_results: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch emails from Gmail and return as DataFrame\n",
        "\n",
        "        Args:\n",
        "            query: Gmail search query\n",
        "            max_results: Maximum number of emails to fetch\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Email data\n",
        "        \"\"\"\n",
        "        processed_emails = []\n",
        "\n",
        "        try:\n",
        "            # Search for emails\n",
        "            search_params = {\"query\": query, \"max_results\": max_results}\n",
        "            search_results = self.search_tool.run(search_params)\n",
        "\n",
        "            print(f\"üìß Found {len(search_results)} emails, fetching details...\")\n",
        "\n",
        "            # Process each email\n",
        "            for summary in search_results:\n",
        "                message_id = summary.get('id')\n",
        "                message_detail = self.api_resource.users().messages().get(\n",
        "                    userId='me',\n",
        "                    id=message_id,\n",
        "                    format='full'\n",
        "                ).execute()\n",
        "\n",
        "                payload = message_detail.get('payload', {})\n",
        "                headers = payload.get('headers', [])\n",
        "\n",
        "                # Extract header information\n",
        "                subject = next((h['value'] for h in headers if h['name'].lower() == 'subject'), 'N/A')\n",
        "                sender = next((h['value'] for h in headers if h['name'].lower() == 'from'), 'N/A')\n",
        "                to = next((h['value'] for h in headers if h['name'].lower() == 'to'), 'N/A')\n",
        "                cc = next((h['value'] for h in headers if h['name'].lower() == 'cc'), 'N/A')\n",
        "                date_str = next((h['value'] for h in headers if h['name'].lower() == 'date'), None)\n",
        "\n",
        "                # Parse date\n",
        "                try:\n",
        "                    email_date = parse(date_str) if date_str else None\n",
        "                except:\n",
        "                    email_date = None\n",
        "\n",
        "                # Check labels and read status\n",
        "                labels = message_detail.get('labelIds', [])\n",
        "                is_unread = 'UNREAD' in labels\n",
        "\n",
        "                # Extract content - FULL TEXT, no truncation for security\n",
        "                contents = self.get_email_contents(payload)\n",
        "\n",
        "                # Recursive attachment check\n",
        "                has_attachment = False\n",
        "                attachment_names = []\n",
        "\n",
        "                def check_attachments(part):\n",
        "                    \"\"\"Recursive attachment check\"\"\"\n",
        "                    if part.get('filename'):\n",
        "                        return True, part.get('filename')\n",
        "                    if 'parts' in part:\n",
        "                        for subpart in part['parts']:\n",
        "                            has_att, filename = check_attachments(subpart)\n",
        "                            if has_att:\n",
        "                                return True, filename\n",
        "                    return False, None\n",
        "\n",
        "                if 'parts' in payload:\n",
        "                    for part in payload['parts']:\n",
        "                        has_att, filename = check_attachments(part)\n",
        "                        if has_att:\n",
        "                            has_attachment = True\n",
        "                            if filename:\n",
        "                                attachment_names.append(filename)\n",
        "\n",
        "                # Add email data - storing FULL content\n",
        "                processed_emails.append({\n",
        "                    'id': message_id,\n",
        "                    'is_unread': is_unread,\n",
        "                    'date': email_date,\n",
        "                    'from': sender,\n",
        "                    'to': to,\n",
        "                    'cc': cc,\n",
        "                    'labels': labels,\n",
        "                    'subject': subject,\n",
        "                    'body_text': contents['text'],  # Full text for analysis\n",
        "                    'body_html': contents['html'],  # Full HTML for security checks\n",
        "                    'has_attachment': has_attachment,\n",
        "                    'attachment_names': attachment_names\n",
        "                })\n",
        "\n",
        "            # Create DataFrame\n",
        "            if processed_emails:\n",
        "                df = pd.DataFrame(processed_emails)\n",
        "                df = df.sort_values(by='date', ascending=False, na_position='last').reset_index(drop=True)\n",
        "                print(f\"‚úÖ Successfully processed {len(df)} emails\")\n",
        "                return df\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No emails found\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Email fetch error: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def save_to_csv(self, df: pd.DataFrame, filename: str = 'fetched_emails.csv'):\n",
        "        \"\"\"\n",
        "        Save DataFrame to CSV file\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to save\n",
        "            filename: Output filename\n",
        "        \"\"\"\n",
        "        if not df.empty:\n",
        "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "            print(f\"‚úÖ DataFrame successfully saved to '{filename}'\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No data to save\")\n",
        "\n",
        "# Test the fetcher\n",
        "email_fetcher = EmailFetcher()\n",
        "print(\"‚úÖ EmailFetcher successfully created\")"
      ],
      "metadata": {
        "id": "6xr_lbsZrvJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70717ed-deee-4765-bada-b8ee2f797518"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ EmailFetcher successfully created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Security Analysis Module**"
      ],
      "metadata": {
        "id": "ogkvJZGBiUPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Security analysis module with hybrid LLM integration\n",
        "class SecurityAnalyser:\n",
        "    \"\"\"Email security analysis class with hybrid deterministic + LLM approach\"\"\"\n",
        "\n",
        "    def __init__(self, llm_model=None):\n",
        "        \"\"\"Initialise threat patterns and LLM integration\"\"\"\n",
        "\n",
        "        # Optional LLM for domain assessment\n",
        "        self.llm = llm_model or ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # Using lighter model for quick assessments\n",
        "            temperature=0.1,  # Low temperature for consistent security decisions\n",
        "        )\n",
        "\n",
        "        # Phishing/Scam patterns\n",
        "        self.phishing_patterns = [\n",
        "            # URL patterns\n",
        "            r'bit\\.ly|tinyurl|short\\.link|clck\\.ru',  # URL shorteners\n",
        "            r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}',  # IP addresses\n",
        "            r'@[^@]*@',  # Double @ signs\n",
        "\n",
        "            # Word patterns\n",
        "            r'urgent.{0,20}action.{0,20}required',\n",
        "            r'verify.{0,20}account.{0,20}immediately',\n",
        "            r'suspended.{0,20}account',\n",
        "            r'click.{0,20}here.{0,20}immediately',\n",
        "            r'limited.{0,20}time.{0,20}offer',\n",
        "            r'congratulations.{0,20}won',\n",
        "            r'claim.{0,20}prize',\n",
        "            r'tax.{0,20}refund',\n",
        "            r'nigerian?.{0,20}prince',\n",
        "        ]\n",
        "\n",
        "        # Prompt injection patterns\n",
        "        self.injection_patterns = [\n",
        "            r'ignore.{0,20}previous.{0,20}instructions',\n",
        "            r'disregard.{0,20}all.{0,20}prior',\n",
        "            r'forget.{0,20}everything',\n",
        "            r'new.{0,20}instructions.{0,20}follow',\n",
        "            r'system.{0,20}prompt.{0,20}override',\n",
        "            r'admin.{0,20}mode',\n",
        "            r'developer.{0,20}mode',\n",
        "            r'bypass.{0,20}security',\n",
        "            r'<script',  # XSS attempts\n",
        "            r'javascript:',\n",
        "            r'eval\\(',\n",
        "            r'onerror=',\n",
        "        ]\n",
        "\n",
        "        # Sample suspicious domains (20 examples)\n",
        "        self.suspicious_domains = [\n",
        "            'phishing-site.com', 'fake-bank.net', 'suspicious-domain.org',\n",
        "            'secure-verify.com', 'account-update.net', 'paypal-verify.com',\n",
        "            'amazon-security.net', 'microsoft-alert.com', 'google-secure.org',\n",
        "            'apple-id-verify.com', 'netflix-billing.net', 'facebook-alert.org',\n",
        "            'instagram-verify.com', 'linkedin-update.net', 'dropbox-alert.com',\n",
        "            'adobe-verify.org', 'office365-alert.net', 'icloud-verify.com',\n",
        "            'ebay-security.net', 'steam-verify.org'\n",
        "        ]\n",
        "\n",
        "        # Sample trusted domains (20 examples)\n",
        "        self.trusted_domains = [\n",
        "            'gmail.com', 'outlook.com', 'yahoo.com', 'google.com',\n",
        "            'microsoft.com', 'apple.com', 'amazon.com', 'facebook.com',\n",
        "            'linkedin.com', 'github.com', 'stackoverflow.com', 'twitter.com',\n",
        "            'paypal.com', 'ebay.com', 'netflix.com', 'spotify.com',\n",
        "            'adobe.com', 'dropbox.com', 'slack.com', 'zoom.us'\n",
        "        ]\n",
        "\n",
        "    def assess_domain_with_llm(self, domain: str, email_context: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Use LLM to assess domain trustworthiness\n",
        "\n",
        "        Args:\n",
        "            domain: Domain to assess\n",
        "            email_context: Email context for better assessment\n",
        "\n",
        "        Returns:\n",
        "            Dict with assessment results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare prompt for domain assessment\n",
        "            prompt = f\"\"\"Analyse this email sender domain for security risks.\n",
        "\n",
        "Domain: {domain}\n",
        "Email Subject: {email_context.get('subject', 'N/A')}\n",
        "Sender Full Address: {email_context.get('from', 'N/A')}\n",
        "\n",
        "Based on the domain name pattern, common phishing tactics, and email context, assess if this domain appears:\n",
        "1. SUSPICIOUS (likely phishing/scam)\n",
        "2. TRUSTED (legitimate business/service)\n",
        "3. UNKNOWN (cannot determine)\n",
        "\n",
        "Consider:\n",
        "- Does the domain mimic known brands?\n",
        "- Does it use suspicious patterns (extra words, misspellings)?\n",
        "- Is it a legitimate business domain?\n",
        "\n",
        "Respond with ONLY one word: SUSPICIOUS, TRUSTED, or UNKNOWN\n",
        "\n",
        "CRITICAL NOTE: Try not to mark as ‚ÄòUNKNOWN‚Äô as much as possible.\n",
        "\n",
        "Decision:\"\"\"\n",
        "\n",
        "            # Get LLM assessment\n",
        "            response = self.llm.invoke(prompt)\n",
        "            assessment = response.content.strip().upper()\n",
        "\n",
        "            # Validate response\n",
        "            if assessment not in ['SUSPICIOUS', 'TRUSTED', 'UNKNOWN']:\n",
        "                assessment = 'UNKNOWN'\n",
        "\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': assessment,\n",
        "                'confidence': 'medium'  # LLM assessments get medium confidence\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è LLM assessment failed for {domain}: {e}\")\n",
        "            return {\n",
        "                'domain': domain,\n",
        "                'llm_assessment': 'UNKNOWN',\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "    def check_phishing_indicators(self, email_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Check for phishing indicators with hybrid approach\"\"\"\n",
        "        indicators = []\n",
        "        risk_score = 0\n",
        "        domain_assessment = {}\n",
        "\n",
        "        # Analyse full text\n",
        "        text = f\"{email_data.get('subject', '')} {email_data.get('body_text', '')} {email_data.get('body_html', '')}\".lower()\n",
        "\n",
        "        # Pattern checking\n",
        "        for pattern in self.phishing_patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                indicators.append(f\"Suspicious pattern detected: {pattern}\")\n",
        "                risk_score += 20\n",
        "\n",
        "        # Urgency words analysis\n",
        "        urgency_words = ['urgent', 'immediate', 'expire', 'suspend', 'limited time']\n",
        "        urgency_count = sum(1 for word in urgency_words if word in text)\n",
        "        if urgency_count > 2:\n",
        "            indicators.append(f\"High urgency level detected ({urgency_count} keywords)\")\n",
        "            risk_score += urgency_count * 10\n",
        "\n",
        "        # Enhanced sender analysis with hybrid approach\n",
        "        sender = email_data.get('from', '')\n",
        "        sender_domain = sender.split('@')[-1].split('>')[0] if '@' in sender else ''\n",
        "\n",
        "        if sender_domain:\n",
        "            # Check display name vs actual email\n",
        "            if '<' in sender and '>' in sender:\n",
        "                display_name = sender.split('<')[0].strip()\n",
        "                actual_email = sender.split('<')[1].split('>')[0]\n",
        "\n",
        "                # Check if display name contains different email\n",
        "                if '@' in display_name:\n",
        "                    indicators.append(\"Display name contains different email address\")\n",
        "                    risk_score += 30\n",
        "\n",
        "            # Domain trust assessment - Hybrid approach\n",
        "            if sender_domain in self.suspicious_domains:\n",
        "                # Known suspicious domain\n",
        "                indicators.append(f\"Known suspicious domain: {sender_domain}\")\n",
        "                risk_score += 30\n",
        "                domain_assessment = {'status': 'suspicious', 'source': 'blacklist'}\n",
        "\n",
        "            elif sender_domain in self.trusted_domains:\n",
        "                # Known trusted domain\n",
        "                indicators.append(f\"Trusted domain: {sender_domain}\")\n",
        "                risk_score -= 20  # Reduce risk score\n",
        "                risk_score = max(0, risk_score)  # Don't go below 0\n",
        "                domain_assessment = {'status': 'trusted', 'source': 'whitelist'}\n",
        "\n",
        "            else:\n",
        "                # Unknown domain - use LLM for assessment\n",
        "                llm_result = self.assess_domain_with_llm(sender_domain, email_data)\n",
        "                domain_assessment = {'status': llm_result['llm_assessment'].lower(), 'source': 'llm'}\n",
        "\n",
        "                if llm_result['llm_assessment'] == 'SUSPICIOUS':\n",
        "                    indicators.append(f\"LLM assessed domain as suspicious: {sender_domain}\")\n",
        "                    risk_score += 30  # Same weight as blacklist\n",
        "\n",
        "                elif llm_result['llm_assessment'] == 'TRUSTED':\n",
        "                    indicators.append(f\"LLM assessed domain as potentially trusted: {sender_domain}\")\n",
        "                    risk_score -= 10  # Half the reduction of whitelist (less confidence)\n",
        "                    risk_score = max(0, risk_score)\n",
        "\n",
        "                else:  # UNKNOWN\n",
        "                    indicators.append(f\"Domain assessment inconclusive: {sender_domain}\")\n",
        "                    risk_score += 10  # Increase the Risk Score slightly\n",
        "\n",
        "        # Attachment analysis\n",
        "        if email_data.get('has_attachment'):\n",
        "            attachments = email_data.get('attachment_names', [])\n",
        "            dangerous_extensions = ['.exe', '.zip', '.rar', '.bat', '.cmd', '.scr', '.vbs']\n",
        "\n",
        "            for att in attachments:\n",
        "                if any(att.lower().endswith(ext) for ext in dangerous_extensions):\n",
        "                    indicators.append(f\"Dangerous file extension detected: {att}\")\n",
        "                    risk_score += 40\n",
        "\n",
        "        return {\n",
        "            'indicators': indicators,\n",
        "            'risk_score': min(risk_score, 100),\n",
        "            'risk_level': self._calculate_risk_level(risk_score),\n",
        "            'domain_assessment': domain_assessment\n",
        "        }\n",
        "\n",
        "    def check_url_safety(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Check URL safety in email content\"\"\"\n",
        "        urls = re.findall(r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+', text)\n",
        "        suspicious_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            # Check for URL shorteners\n",
        "            if any(short in url.lower() for short in ['bit.ly', 'tinyurl', 'short.link']):\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'URL shortener detected - could hide malicious destination'\n",
        "                })\n",
        "\n",
        "            # Check for IP addresses instead of domains\n",
        "            if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url):\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'Contains IP address instead of domain name'\n",
        "                })\n",
        "\n",
        "            # Check for homograph attacks (similar looking characters)\n",
        "            if any(char in url for char in ['–∞', '–µ', '–æ', '—Ä', '—Å', '—É', '—Ö']):  # Cyrillic chars\n",
        "                suspicious_urls.append({\n",
        "                    'url': url,\n",
        "                    'reason': 'Possible homograph attack - contains lookalike characters'\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'total_urls': len(urls),\n",
        "            'suspicious_urls': suspicious_urls,\n",
        "            'risk_level': 'high' if suspicious_urls else 'low'\n",
        "        }\n",
        "\n",
        "    def check_prompt_injection(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Check for prompt injection attempts\"\"\"\n",
        "        injections_found = []\n",
        "\n",
        "        for pattern in self.injection_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                injections_found.append({\n",
        "                    'pattern': pattern,\n",
        "                    'matches': matches[:3]  # First 3 matches\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'injection_detected': len(injections_found) > 0,\n",
        "            'injection_patterns': injections_found,\n",
        "            'risk_level': 'critical' if len(injections_found) > 2 else\n",
        "                         'high' if len(injections_found) > 0 else 'none'\n",
        "        }\n",
        "\n",
        "    def _calculate_risk_level(self, score: int) -> str:\n",
        "        \"\"\"Calculate risk level from score\"\"\"\n",
        "        if score >= 70:\n",
        "            return 'critical'\n",
        "        elif score >= 50:\n",
        "            return 'high'\n",
        "        elif score >= 30:\n",
        "            return 'medium'\n",
        "        elif score >= 10:\n",
        "            return 'low'\n",
        "        else:\n",
        "            return 'safe'\n",
        "\n",
        "    def analyse_email_security(self, email_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Complete security analysis with hybrid approach\"\"\"\n",
        "\n",
        "        # Run all analyses using FULL text\n",
        "        full_text = f\"{email_data.get('subject', '')} {email_data.get('body_text', '')} {email_data.get('body_html', '')}\"\n",
        "\n",
        "        url_analysis = self.check_url_safety(full_text)\n",
        "        phishing_analysis = self.check_phishing_indicators(email_data)\n",
        "        injection_analysis = self.check_prompt_injection(full_text)\n",
        "\n",
        "        # Calculate overall risk score\n",
        "        overall_risk_score = phishing_analysis['risk_score']\n",
        "\n",
        "        if url_analysis['risk_level'] == 'high':\n",
        "            overall_risk_score = min(overall_risk_score + 30, 100)\n",
        "\n",
        "        if injection_analysis['risk_level'] == 'critical':\n",
        "            overall_risk_score = min(overall_risk_score + 50, 100)\n",
        "        elif injection_analysis['risk_level'] == 'high':\n",
        "            overall_risk_score = min(overall_risk_score + 30, 100)\n",
        "\n",
        "        return {\n",
        "            'email_id': email_data.get('id'),\n",
        "            'subject': email_data.get('subject'),\n",
        "            'sender': email_data.get('from'),\n",
        "            'overall_risk_score': overall_risk_score,\n",
        "            'overall_risk_level': self._calculate_risk_level(overall_risk_score),\n",
        "            'domain_assessment': phishing_analysis.get('domain_assessment', {}),\n",
        "            'url_analysis': url_analysis,\n",
        "            'phishing_analysis': phishing_analysis,\n",
        "            'injection_analysis': injection_analysis,\n",
        "            'recommendations': self._generate_recommendations(\n",
        "                overall_risk_score,\n",
        "                phishing_analysis,\n",
        "                injection_analysis\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def _generate_recommendations(self, risk_score: int, phishing: Dict, injection: Dict) -> List[str]:\n",
        "        \"\"\"Generate security recommendations\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        if risk_score >= 70:\n",
        "            recommendations.append(\"‚õî CRITICAL: Do NOT open this email - DELETE immediately!\")\n",
        "            recommendations.append(\"üö® Report to IT security team\")\n",
        "        elif risk_score >= 50:\n",
        "            recommendations.append(\"‚ö†Ô∏è HIGH RISK: Do not click any links\")\n",
        "            recommendations.append(\"üìß Verify sender identity independently\")\n",
        "        elif risk_score >= 30:\n",
        "            recommendations.append(\"‚ö° CAUTION: Suspicious content detected\")\n",
        "\n",
        "        if injection['injection_detected']:\n",
        "            recommendations.append(\"ü§ñ PROMPT INJECTION detected - do not copy to AI systems\")\n",
        "\n",
        "        if phishing['indicators']:\n",
        "            recommendations.append(\"üé£ Phishing indicators detected - do not share personal information\")\n",
        "\n",
        "        # Add domain-specific recommendation\n",
        "        domain_assessment = phishing.get('domain_assessment', {})\n",
        "        if domain_assessment.get('source') == 'llm' and domain_assessment.get('status') == 'suspicious':\n",
        "            recommendations.append(\"üîç AI assessment suggests sender domain may be suspicious\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Test the analyser with hybrid approach\n",
        "security_analyser = SecurityAnalyser()\n",
        "print(\"‚úÖ SecurityAnalyser with hybrid approach successfully created\")"
      ],
      "metadata": {
        "id": "nb4QQYchrvMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071d3c30-81bc-4185-f13e-84c6da7b4573"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SecurityAnalyser with hybrid approach successfully created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zCFGmafDlHEn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVV0o9RprvPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-i0ev0sIrvSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClbIRx_nrvVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDzDkt-srvYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yerKyNNfrvby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6sM-d7ervfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbGAL-0WrviH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k89h_FMgrvlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OjizjfQcrvoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4N6RFeDDrvru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWvXx8_7rvui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}